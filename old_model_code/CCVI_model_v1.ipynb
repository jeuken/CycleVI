{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e036cf9-0a5e-4e3b-9696-b84f3cf5195d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740d7c88-003d-44f1-96f0-e95f3dea3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piamozdzanowski/anaconda3/envs/spring_2025/lib/python3.12/site-packages/docrep/decorators.py:43: SyntaxWarning: 'param_categorical_covariate_keys' is not a valid key!\n",
      "  doc = func(self, args[0].__doc__, *args[1:], **kwargs)\n",
      "/Users/piamozdzanowski/anaconda3/envs/spring_2025/lib/python3.12/site-packages/docrep/decorators.py:43: SyntaxWarning: 'param_continuous_covariate_keys' is not a valid key!\n",
      "  doc = func(self, args[0].__doc__, *args[1:], **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from typing import Literal\n",
    "\n",
    "import scvi\n",
    "import torch\n",
    "from scvi import REGISTRY_KEYS\n",
    "from scvi.module.base import (\n",
    "    BaseModuleClass,\n",
    "    LossOutput,\n",
    "    auto_move_data,\n",
    ")\n",
    "from torch.distributions import NegativeBinomial, Normal\n",
    "from torch.distributions import kl_divergence as kl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Dirichlet, NegativeBinomial\n",
    "from scvi.model.base import UnsupervisedTrainingMixin, BaseModelClass\n",
    "from scvi.data import AnnDataManager\n",
    "from scvi.data.fields import (\n",
    "    LayerField,\n",
    "    CategoricalObsField,\n",
    "    NumericalObsField,\n",
    "    CategoricalJointObsField,\n",
    "    NumericalJointObsField,\n",
    ")\n",
    "from anndata import AnnData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa964d77-c46e-4d66-9d55-32b678f6a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run with scvi-tools version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "scvi.settings.seed = 0\n",
    "print(\"Last run with scvi-tools version:\", scvi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a7ff5f5-5257-49f2-95ea-7071b80b883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "save_dir = \"cell_cycle_test_model\"\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs={\"facecolor\": \"w\"}\n",
    "%config InlineBackend.figure_format=\"retina\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5675f20-fbe0-460d-abde-3d5b347ad545",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d3887-c301-4bfe-902f-04137b13f229",
   "metadata": {},
   "source": [
    "This class defines a fully connected neural network with a given number of inputs and outputs,\n",
    "a single hidden layer with 128 neurons and ReLU activation, and the chosen final activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1b0afb-e396-426b-b741-5e8eb7c8226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet(torch.nn.Module):\n",
    "# Initialization of network\n",
    "    def __init__(\n",
    "        # 1. Defining the inputs\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        link_var: Literal[\"exp\", \"none\", \"softmax\"],\n",
    "    ):\n",
    "        \"\"\"Encodes data of ``n_input`` dimensions into a space of ``n_output`` dimensions.\n",
    "\n",
    "        Uses a one layer fully-connected neural network with 128 hidden nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_input\n",
    "            The dimensionality of the input.\n",
    "        n_output\n",
    "            The dimensionality of the output.\n",
    "        link_var\n",
    "            The final non-linearity.\n",
    "        \"\"\"\n",
    "        # 2. Defining the architecture\n",
    "        super().__init__() # Initialize the parent class torch.nn.Module\n",
    "        self.neural_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_input, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, n_output),\n",
    "        )\n",
    "        # 3. Defining the final activation function\n",
    "        self.transformation = None\n",
    "        if link_var == \"softmax\":\n",
    "            self.transformation = torch.nn.Softmax(dim=-1) # applies softmax along the last dimension\n",
    "        elif link_var == \"exp\":\n",
    "            self.transformation = torch.exp # exp doesn’t need dim because it applies to each element independently.\n",
    "\n",
    "# Forward pass\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output = self.neural_net(x)\n",
    "        if self.transformation:\n",
    "            output = self.transformation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc75dd7-11fa-4f04-9694-d1109b8fc51a",
   "metadata": {},
   "source": [
    "Exploring this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e1deb0-302b-4cba-a7f4-db672a643f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNeuralNet(\n",
       "  (neural_net): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       "  (transformation): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_neural_net = MyNeuralNet(10, 4, \"softmax\") # 100 input features, 10 output features with softmax activation\n",
    "my_neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d36eab2-8ca5-4844-990a-e3b633b24e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1545, 0.1928, 0.2447, 0.4081],\n",
       "        [0.2039, 0.2265, 0.3178, 0.2517]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe that the output sums to 1 and are positive!\n",
    "x = torch.randn((2, 10))\n",
    "my_neural_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e3d99-e8f1-4985-a375-a20b0ffdb0c8",
   "metadata": {},
   "source": [
    "## Crafting the Module in vanilla PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff66d6f-d1c9-4e2d-ae3a-5ea2ddb8be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(BaseModuleClass):\n",
    "    \"\"\"Skeleton Variational auto-encoder model.\n",
    "\n",
    "    Here we implement a basic version of scVI's underlying VAE [Lopez18]_.\n",
    "    This implementation is for instructional purposes only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_latent: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # in the init, we create the parameters of our elementary stochastic computation unit.\n",
    "\n",
    "        # First, we setup the parameters of the generative model\n",
    "        self.decoder = MyNeuralNet(n_latent, n_input, \"softmax\")\n",
    "        self.log_theta = torch.nn.Parameter(torch.randn(n_input)) # initializes a trainable parameter with random values from a normal distribution\n",
    "\n",
    "        # Second, we setup the parameters of the variational distribution\n",
    "        self.mean_encoder = MyNeuralNet(n_input, n_latent, \"none\") # the mean of a distribution can take any real value\n",
    "        self.var_encoder = MyNeuralNet(n_input, n_latent, \"exp\") # applying exp ensures that variance is always positive\n",
    "\n",
    "    def _get_inference_input(self, tensors: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Parse the dictionary to get appropriate args\"\"\"\n",
    "        # This function takes a dictionary of tensors and extracts the required input tensor (raw counts) for the inference step, formatting it into a new dictionary.\n",
    "        return {\"x\": tensors[REGISTRY_KEYS.X_KEY]}\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(self, x: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        # log the input to the variational distribution for numerical stability\n",
    "        x_ = torch.log1p(x)\n",
    "        # get variational parameters via the encoder networks\n",
    "        qz_m = self.mean_encoder(x_)\n",
    "        qz_v = self.var_encoder(x_)\n",
    "        # get one sample to feed to the generative model\n",
    "        # under the hood here is the Reparametrization trick (Rsample)\n",
    "        z = Normal(qz_m, torch.sqrt(qz_v)).rsample()\n",
    "\n",
    "        return {\"qz_m\": qz_m, \"qz_v\": qz_v, \"z\": z}\n",
    "\n",
    "    def _get_generative_input(\n",
    "        self, tensors: dict[str, torch.Tensor], inference_outputs: dict[str, torch.Tensor]\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"z\": inference_outputs[\"z\"],\n",
    "            \"library\": torch.sum(tensors[REGISTRY_KEYS.X_KEY], dim=1, keepdim=True),\n",
    "        }\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z: torch.Tensor, library: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "        # get the \"normalized\" mean of the negative binomial\n",
    "        px_scale = self.decoder(z)\n",
    "        # get the mean of the negative binomial\n",
    "        px_rate = library * px_scale\n",
    "        # get the dispersion parameter\n",
    "        theta = torch.exp(self.log_theta)\n",
    "\n",
    "        return {\n",
    "            \"px_scale\": px_scale,\n",
    "            \"theta\": theta,\n",
    "            \"px_rate\": px_rate,\n",
    "        }\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],\n",
    "        inference_outputs: dict[str, torch.Tensor],\n",
    "        generative_outputs: dict[str, torch.Tensor],\n",
    "    ) -> LossOutput:\n",
    "        # here, we would like to form the ELBO. There are two terms:\n",
    "        #   1. one that pertains to the likelihood of the data\n",
    "        #   2. one that pertains to the variational distribution\n",
    "        # so we extract all the required information\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        px_rate = generative_outputs[\"px_rate\"]\n",
    "        theta = generative_outputs[\"theta\"]\n",
    "        qz_m = inference_outputs[\"qz_m\"]\n",
    "        qz_v = inference_outputs[\"qz_v\"]\n",
    "\n",
    "        # term 1\n",
    "        # the pytorch NB distribution uses a different parameterization\n",
    "        # so we must apply a quick transformation (included in scvi-tools, but here we use the\n",
    "        # pytorch code)\n",
    "        nb_logits = (px_rate + 1e-4).log() - (theta + 1e-4).log()\n",
    "        log_lik = NegativeBinomial(total_count=theta, logits=nb_logits).log_prob(x).sum(dim=-1)\n",
    "\n",
    "        # term 2\n",
    "        prior_dist = Normal(torch.zeros_like(qz_m), torch.ones_like(qz_v))\n",
    "        var_post_dist = Normal(qz_m, torch.sqrt(qz_v))\n",
    "        kl_divergence = kl(var_post_dist, prior_dist).sum(dim=1)\n",
    "\n",
    "        elbo = log_lik - kl_divergence\n",
    "        loss = torch.mean(-elbo)\n",
    "        return LossOutput(\n",
    "            loss=loss,\n",
    "            reconstruction_loss=-log_lik,\n",
    "            kl_local=kl_divergence,\n",
    "            kl_global=0.0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9a4ef9c-8739-46a2-aeb9-c76178d68879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (decoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=100, bias=True)\n",
       "    )\n",
       "    (transformation): Softmax(dim=-1)\n",
       "  )\n",
       "  (mean_encoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (var_encoder): MyNeuralNet(\n",
       "    (neural_net): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try creating a module and see the description:\n",
    "MyModule(100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56fbcd-ace2-479d-8691-b90d5bdfbf07",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bdcfd1-c03e-4e38-8f15-ff8e75fb7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCVI(UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"single-cell Variational Inference [Lopez18]_.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_latent: int = 10,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "\n",
    "        self.module = VAE(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_latent=n_latent,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            f\"SCVI Model with the following params: \\nn_latent: {n_latent}\"\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    @classmethod\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        batch_key: str | None = None,\n",
    "        layer: str | None = None,\n",
    "        **kwargs,\n",
    "    ) -> AnnData | None:\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),\n",
    "            # Dummy fields required for VAE class.\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, None),\n",
    "            NumericalObsField(REGISTRY_KEYS.SIZE_FACTOR_KEY, None, required=False),\n",
    "            CategoricalJointObsField(REGISTRY_KEYS.CAT_COVS_KEY, None),\n",
    "            NumericalJointObsField(REGISTRY_KEYS.CONT_COVS_KEY, None),\n",
    "        ]\n",
    "        adata_manager = AnnDataManager(fields=anndata_fields, setup_method_args=setup_method_args)\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08c8eb-8256-49bc-8c10-fe6d74b60715",
   "metadata": {},
   "source": [
    "## Model with Dirichlet & NB Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c11f4fa-301a-4fe8-9476-268ff46fd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CellCycleVAE(UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"Single-cell Variational Inference model with Dirichlet and Negative Binomial decoders.\"\"\"\n",
    "\n",
    "    def __init__(self, adata: AnnData, n_latent: int = 10, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata : AnnData\n",
    "            Annotated data matrix.\n",
    "        n_latent : int\n",
    "            Dimensionality of the latent space (default: 10).\n",
    "        **model_kwargs : dict\n",
    "            Additional arguments for the model.\n",
    "        \"\"\"\n",
    "        super().__init__(adata)\n",
    "\n",
    "        # Define the main generative module\n",
    "        self.module = Cycle_VAE(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_latent=n_latent,\n",
    "            n_phases=4,  # 4 cell cycle phases: G0, G1, S, G2M\n",
    "            **model_kwargs,\n",
    "        )\n",
    "\n",
    "        # Store model parameters\n",
    "        self._model_summary_string = (\n",
    "            f\"CellCycleVAE Model with the following params: \\nn_latent: {n_latent}\"\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    @classmethod\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        batch_key: str | None = None,\n",
    "        layer: str | None = None,\n",
    "        **kwargs,\n",
    "    ) -> AnnData | None:\n",
    "        \"\"\"\n",
    "        Prepares an AnnData object for use with CellCycleVAE.\n",
    "        \"\"\"\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),\n",
    "            # Dummy fields required for the VAE class\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, None),\n",
    "            NumericalObsField(REGISTRY_KEYS.SIZE_FACTOR_KEY, None, required=False),\n",
    "            CategoricalJointObsField(REGISTRY_KEYS.CAT_COVS_KEY, None),\n",
    "            NumericalJointObsField(REGISTRY_KEYS.CONT_COVS_KEY, None),\n",
    "        ]\n",
    "\n",
    "        adata_manager = AnnDataManager(fields=anndata_fields, setup_method_args=setup_method_args)\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3c7d337-65cf-405b-8062-43328b0682e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cycle_VAE(nn.Module):\n",
    "    \"\"\"VAE with a Dirichlet decoder for cell cycle phase and a Negative Binomial decoder for gene expression.\"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_latent=10, n_phases=4, n_batch=0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_input : int\n",
    "            Number of genes (input features).\n",
    "        n_latent : int\n",
    "            Dimensionality of the latent space (default: 10).\n",
    "        n_phases : int\n",
    "            Number of cell cycle phases (default: 4 for G0, G1, S, G2M).\n",
    "        n_batch : int\n",
    "            Number of batch effect categories (default: 0, no batch correction).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: Maps gene expression to latent space (z)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_input, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_latent * 2),  # Outputs mean & log variance for z\n",
    "        )\n",
    "\n",
    "        # Dirichlet Decoder: Maps latent space z to cell cycle phase distribution π\n",
    "        self.dirichlet_decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_phases),\n",
    "            nn.Softplus(),  # Ensures positive values for Dirichlet parameters\n",
    "        )\n",
    "\n",
    "        # Negative Binomial Decoder: Maps π to gene expression parameters (μ)\n",
    "        self.nb_decoder = nn.Sequential(\n",
    "            nn.Linear(n_phases, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_input),  # Mean gene expression for NB distribution\n",
    "        )\n",
    "\n",
    "        # Dispersion parameter for Negative Binomial (theta)\n",
    "        self.theta = nn.Parameter(torch.randn(n_input))\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes input gene expression x into latent space z.\"\"\"\n",
    "        q = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(q, 2, dim=-1)  # Split into mean & log variance\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        return mu, std\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        \"\"\"Reparametrization trick for sampling from q(z|x).\"\"\"\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the VAE model.\"\"\"\n",
    "        \n",
    "        # 1. Encode x → latent space z\n",
    "        mu, std = self.encode(x)\n",
    "        z = self.reparameterize(mu, std)\n",
    "\n",
    "        # 2. Decode z → cell cycle phase distribution π (Dirichlet)\n",
    "        alpha = self.dirichlet_decoder(z) + 1e-6  # Add small constant for stability\n",
    "        pi = Dirichlet(alpha).rsample()  # Sample π from Dirichlet(α)\n",
    "\n",
    "        # 3. Decode π → gene expression parameters μ (NB decoder)\n",
    "        mu_nb = self.nb_decoder(pi)\n",
    "        mu_nb = torch.exp(mu_nb)  # Ensure positive gene expression means\n",
    "\n",
    "        # 4. Compute Negative Binomial output\n",
    "        theta = torch.exp(self.theta)  # Ensure dispersion is positive\n",
    "        nb_dist = NegativeBinomial(total_count=theta, probs=mu_nb / (mu_nb + theta))\n",
    "\n",
    "        return nb_dist, mu, std, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "609889b8-b461-4f04-9f10-bebdf7826f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 400 × 100\n",
       "    obs: 'batch', 'labels'\n",
       "    uns: 'protein_names'\n",
       "    obsm: 'protein_expression', 'accessibility'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = scvi.data.synthetic_iid()\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1db0728-2147-4c05-a7c2-32cded92f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adata UUID (assigned by setup_anndata): 9293e1c4-bcc7-4bf6-b27b-9bc6d9152318\n",
      "AnnDataManager: <scvi.data._manager.AnnDataManager object at 0x2869ef380>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cycle_VAE.__init__() got an unexpected keyword argument 'n_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madata UUID (assigned by setup_anndata): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madata\u001b[38;5;241m.\u001b[39muns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_scvi_uuid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnDataManager: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCellCycleVAE\u001b[38;5;241m.\u001b[39m_setup_adata_manager_store[adata\u001b[38;5;241m.\u001b[39muns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_scvi_uuid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m CellCycleVAE(adata)\n\u001b[1;32m      5\u001b[0m model\n",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m, in \u001b[0;36mCellCycleVAE.__init__\u001b[0;34m(self, adata, n_latent, **model_kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(adata)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define the main generative module\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m Cycle_VAE(\n\u001b[1;32m     19\u001b[0m     n_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_vars\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m     n_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m     n_latent\u001b[38;5;241m=\u001b[39mn_latent,\n\u001b[1;32m     22\u001b[0m     n_phases\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,  \u001b[38;5;66;03m# 4 cell cycle phases: G0, G1, S, G2M\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Store model parameters\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_summary_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCellCycleVAE Model with the following params: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mn_latent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_latent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Cycle_VAE.__init__() got an unexpected keyword argument 'n_batch'"
     ]
    }
   ],
   "source": [
    "CellCycleVAE.setup_anndata(adata, batch_key=\"batch\")\n",
    "print(f\"adata UUID (assigned by setup_anndata): {adata.uns['_scvi_uuid']}\")\n",
    "print(f\"AnnDataManager: {CellCycleVAE._setup_adata_manager_store[adata.uns['_scvi_uuid']]}\")\n",
    "model = CellCycleVAE(adata)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc998a58-c2c1-4cfe-9712-dfc8cf32d155",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cycle_VAE' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spring_2025/lib/python3.12/site-packages/scvi/model/base/_training_mixin.py:146\u001b[0m, in \u001b[0;36mUnsupervisedTrainingMixin.train\u001b[0;34m(self, max_epochs, accelerator, devices, train_size, validation_size, shuffle_set_split, load_sparse_tensor, batch_size, early_stopping, datasplitter_kwargs, plan_kwargs, datamodule, **trainer_kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_cls(\n\u001b[1;32m    137\u001b[0m         datamodule\u001b[38;5;241m.\u001b[39mn_vars,\n\u001b[1;32m    138\u001b[0m         n_batch\u001b[38;5;241m=\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39mn_batch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_kwargs,\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    145\u001b[0m plan_kwargs \u001b[38;5;241m=\u001b[39m plan_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 146\u001b[0m training_plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_plan_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplan_kwargs)\n\u001b[1;32m    148\u001b[0m es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearly_stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m trainer_kwargs[es] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    150\u001b[0m     early_stopping \u001b[38;5;28;01mif\u001b[39;00m es \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m trainer_kwargs\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m trainer_kwargs[es]\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/spring_2025/lib/python3.12/site-packages/scvi/train/_trainingplans.py:207\u001b[0m, in \u001b[0;36mTrainingPlan.__init__\u001b[0;34m(self, module, optimizer, optimizer_creator, lr, update_only_decoder, weight_decay, eps, n_steps_kl_warmup, n_epochs_kl_warmup, reduce_lr_on_plateau, lr_factor, lr_patience, lr_threshold, lr_scheduler_metric, lr_min, max_kl_weight, min_kl_weight, compile, compile_kwargs, **loss_kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# automatic handling of kl weight\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mloss)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_args:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_weight})\n",
      "File \u001b[0;32m~/anaconda3/envs/spring_2025/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cycle_VAE' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "model.train(max_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62790504-5df6-47c5-8fdd-83634c869618",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e51cbe-2a5b-48ed-b044-fd08d9529996",
   "metadata": {},
   "source": [
    "The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8417b-c648-47bd-b958-84a5f2370cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCVI(UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_latent: int = 10,\n",
    "        n_phases=4,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        \n",
    "        super().__init__(adata) # Initialize the parent class with the AnnData object\n",
    "\n",
    "\n",
    "        self.module = CC_VAE(\n",
    "                n_input=self.summary_stats[\"n_vars\"],  # Number of input features (genes)\n",
    "                n_batch=self.summary_stats[\"n_batch\"],  # Number of unique batches in data\n",
    "                n_latent=n_latent,  # Latent space dimensionality\n",
    "                **model_kwargs,  # Pass additional keyword arguments to the VAE model\n",
    "            )\n",
    "\n",
    "        self._model_summary_string = (\n",
    "            f\"CCVI Model with the following params: \\nn_latent: {n_latent}\" # summary string\n",
    "        )\n",
    "\n",
    "        self.init_params_ = self._get_init_params(locals()) # Store the initialization parameters for reproducibility\n",
    "\n",
    "    @classmethod\n",
    "        def setup_anndata(\n",
    "            cls,\n",
    "            adata: AnnData,  # AnnData object to be processed and configured\n",
    "            batch_key: str | None = None,  # Column name in `adata.obs` specifying batch information\n",
    "            layer: str | None = None,  # Layer of `adata` containing raw count data\n",
    "            **kwargs,  # Additional arguments for setup configuration\n",
    "        ) -> AnnData | None:\n",
    "            # Retrieve method arguments for setting up the AnnData object\n",
    "            setup_method_args = cls._get_setup_method_args(**locals())\n",
    "\n",
    "        # Define the fields required for proper data handling in the CCVI model\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),  # Expression data layer\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),  # Batch labels (if provided)\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, None),  # Placeholder for labels (unused)\n",
    "            NumericalObsField(REGISTRY_KEYS.SIZE_FACTOR_KEY, None, required=False),  # Placeholder for size factors (optional)\n",
    "            CategoricalJointObsField(REGISTRY_KEYS.CAT_COVS_KEY, None),  # Placeholder for categorical covariates\n",
    "            NumericalJointObsField(REGISTRY_KEYS.CONT_COVS_KEY, None),  # Placeholder for numerical covariates\n",
    "        ]\n",
    "\n",
    "        # Create an AnnDataManager instance to handle preprocessing and field registration\n",
    "        adata_manager = AnnDataManager(fields=anndata_fields, setup_method_args=setup_method_args)\n",
    "\n",
    "        # Register fields with the AnnData object, applying necessary preprocessing\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "\n",
    "        # Register the manager for future use\n",
    "        cls.register_manager(adata_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218c23b-310c-48b6-b8b7-e881747b2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CC_VAE(nn.Module):\n",
    "    \"\"\"VAE with a Dirichlet decoder for cell cycle phase and a Negative Binomial decoder for gene expression.\"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_latent=10, n_phases=4, n_batch=0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_input : int\n",
    "            Number of genes (input features).\n",
    "        n_latent : int\n",
    "            Dimensionality of the latent space (default: 10).\n",
    "        n_phases : int\n",
    "            Number of cell cycle phases (default: 4 for G0, G1, S, G2M).\n",
    "        n_batch : int\n",
    "            Number of batch effect categories (default: 0, no batch correction).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: Maps gene expression to latent space (z)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_input, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_latent * 2),  # Outputs mean & log variance for z\n",
    "        )\n",
    "\n",
    "        # Dirichlet Decoder: Maps latent space z to cell cycle phase distribution π\n",
    "        self.dirichlet_decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_phases),\n",
    "            nn.Softplus(),  # Ensures positive values for Dirichlet parameters\n",
    "        )\n",
    "\n",
    "        # Negative Binomial Decoder: Maps π to gene expression parameters (μ)\n",
    "        self.nb_decoder = nn.Sequential(\n",
    "            nn.Linear(n_phases, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_input),  # Mean gene expression for NB distribution\n",
    "        )\n",
    "\n",
    "        # Dispersion parameter for Negative Binomial (theta)\n",
    "        self.theta = nn.Parameter(torch.randn(n_input))\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes input gene expression x into latent space z.\"\"\"\n",
    "        q = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(q, 2, dim=-1)  # Split into mean & log variance\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        return mu, std\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        \"\"\"Reparametrization trick for sampling from q(z|x).\"\"\"\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the VAE model.\"\"\"\n",
    "        \n",
    "        # 1. Encode x → latent space z\n",
    "        mu, std = self.encode(x)\n",
    "        z = self.reparameterize(mu, std)\n",
    "\n",
    "        # 2. Decode z → cell cycle phase distribution π (Dirichlet)\n",
    "        alpha = self.dirichlet_decoder(z) + 1e-6  # Add small constant for stability\n",
    "        pi = Dirichlet(alpha).rsample()  # Sample π from Dirichlet(α)\n",
    "\n",
    "        # 3. Decode π → gene expression parameters μ (NB decoder)\n",
    "        mu_nb = self.nb_decoder(pi)\n",
    "        mu_nb = torch.exp(mu_nb)  # Ensure positive gene expression means\n",
    "\n",
    "        # 4. Compute Negative Binomial output\n",
    "        theta = torch.exp(self.theta)  # Ensure dispersion is positive\n",
    "        nb_dist = NegativeBinomial(total_count=theta, probs=mu_nb / (mu_nb + theta))\n",
    "\n",
    "        return nb_dist, mu, std, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c9cfa-f1a4-46c1-8786-77de8c4551db",
   "metadata": {},
   "source": [
    "1. create a class MyNeuralNet(torch.nn.Module)\n",
    "   - given a number of inputs, layers, activations,...\n",
    "   - functions: init and forward\n",
    "2. create a class CC_VAE (BaseModuleClass)\n",
    "   - init: specify the decoders and encoders\n",
    "   - get_generative_input(): selecting the registered tensors from the AnnData, as well as the latent variables (from inference) used in the model\n",
    "- generative(): run the decoders\n",
    "- _get_inference_input(): selecting the registered tensors from the AnnData used in the inference\n",
    "- inference(): run the encoder\n",
    "- loss(): the log-likelihood or its lower bound\n",
    "3. create a class CC_model (BaseModuleClass,UnsupervisedTrainingMixin)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782603cb-5acb-4538-b974-32105571c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # Enables postponed evaluation of type annotations (so classes defined later can be referenced in type hints)\n",
    "import logging                     # Standard Python logging module\n",
    "import warnings                    # Used for issuing warning messages\n",
    "from typing import TYPE_CHECKING  # Used to avoid circular imports at runtime by conditionally importing only for type checking\n",
    "\n",
    "# Importing constants and settings from scvi-tools\n",
    "from scvi import REGISTRY_KEYS, settings\n",
    "from scvi.data import AnnDataManager  # Manages how AnnData is used inside scvi\n",
    "from scvi.data._constants import ADATA_MINIFY_TYPE  # Constants related to minified AnnData types\n",
    "from scvi.data._utils import _get_adata_minify_type  # Utility to get the minified data type\n",
    "from scvi.data.fields import (                      # Definitions for how various fields are extracted from AnnData\n",
    "    CategoricalJointObsField,\n",
    "    CategoricalObsField,\n",
    "    LayerField,\n",
    "    NumericalJointObsField,\n",
    "    NumericalObsField,\n",
    ")\n",
    "\n",
    "from scvi.model._utils import _init_library_size  # Initializes library size parameters (used for normalization)\n",
    "from scvi.model.base import EmbeddingMixin, UnsupervisedTrainingMixin  # Mixins that add embedding and training behaviors\n",
    "from scvi.module import VAE  # The core variational autoencoder module\n",
    "from scvi.utils import setup_anndata_dsp  # Decorator to help document and handle setup_anndata logic\n",
    "\n",
    "# Importing additional mixin classes and base model class\n",
    "from .base import ArchesMixin, BaseMinifiedModeModelClass, RNASeqMixin, VAEMixin\n",
    "\n",
    "# Import type hints only during static type checking\n",
    "if TYPE_CHECKING:\n",
    "    from typing import Literal\n",
    "    from anndata import AnnData  # AnnData is the main data structure for single-cell data\n",
    "\n",
    "# Create a logger for this module\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa35f6-3289-4c95-a476-893295dab8eb",
   "metadata": {},
   "source": [
    "## CC_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b422f29d-6df2-4859-b28c-2264aa684449",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:26\u001b[0;36m\u001b[0m\n\u001b[0;31m    _module_cls = CC_VAE  # Points to the VAE class that this model will use internally\u001b[0m\n\u001b[0m                                                                                        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class CC_model(\n",
    "# 1. CLASS INHERITANCE\n",
    "    EmbeddingMixin,               # Adds methods for getting latent representations\n",
    "    RNASeqMixin,                  # Adds single-cell RNA-seq-specific logic\n",
    "    VAEMixin,                     # Adds methods for working with a VAE model\n",
    "    ArchesMixin,                  # Adds functionality for transfer learning (ARCHES)\n",
    "    UnsupervisedTrainingMixin,   # Adds methods for unsupervised training\n",
    "    BaseMinifiedModeModelClass,  # Adds support for working with memory-efficient minified AnnData\n",
    "):\n",
    "    \n",
    "# 2. CLASS DOCSTRING\n",
    "        \"\"\"single-cell Variational Inference :cite:p:`Lopez18`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData object that has been registered via setup_anndata\n",
    "    n_hidden, n_latent, etc.\n",
    "        VAE architecture and model parameters\n",
    "    Examples\n",
    "    --------\n",
    "    Shows usage with reading data, setting up, training, and getting results\n",
    "    \"\"\"\n",
    "\n",
    "# 3. CLASS ATTRIBUTES\n",
    "    _module_cls = CC_VAE  # Points to the VAE class that this model will use internally \n",
    "    _LATENT_QZM_KEY = \"scvi_latent_qzm\"  # Key for the latent mean in AnnData\n",
    "    _LATENT_QZV_KEY = \"scvi_latent_qzv\"  # Key for the latent variance in AnnData\n",
    "# 4. CONSTRUCTOR\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData | None = None,  # Input data; can be None (if adata is not provided, the model will delay initialization until train is called).\n",
    "        n_hidden: int = 128,           # Hidden units per layer\n",
    "        n_latent: int = 10,            # Dimensionality of latent space\n",
    "        n_layers: int = 1,             # Number of layers in encoder/decoder\n",
    "        dropout_rate: float = 0.1,     # Dropout rate\n",
    "        dispersion: Literal[...] = \"gene\",         # Type of dispersion parameter\n",
    "        gene_likelihood: Literal[...] = \"zinb\",    # Distribution to model gene expression\n",
    "        latent_distribution: Literal[...] = \"normal\",  # Latent distribution type\n",
    "        **kwargs,                      # Any other parameters passed to the VAE\n",
    "    ):\n",
    "        super().__init__(adata)  # Call the constructor of the parent mixin/base classes\n",
    "\n",
    "        # Store parameters in a dictionary\n",
    "        self._module_kwargs = {\n",
    "            \"n_hidden\": n_hidden,\n",
    "            \"n_latent\": n_latent,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"dispersion\": dispersion,\n",
    "            \"gene_likelihood\": gene_likelihood,\n",
    "            \"latent_distribution\": latent_distribution,\n",
    "            **kwargs,\n",
    "        }\n",
    "\n",
    "        # Create a readable summary string\n",
    "        self._model_summary_string = (\n",
    "            \"CC model with the following parameters: \\n\"\n",
    "            f\"n_hidden: {n_hidden}, n_latent: {n_latent}, n_layers: {n_layers}, \"\n",
    "            f\"dropout_rate: {dropout_rate}, dispersion: {dispersion}, \"\n",
    "            f\"gene_likelihood: {gene_likelihood}, latent_distribution: {latent_distribution}.\"\n",
    "        )\n",
    "\n",
    "        # If lazy initialization is enabled (adata is not provided), postpone model creation until training\n",
    "        if self._module_init_on_train:\n",
    "            self.module = None\n",
    "            warnings.warn(\n",
    "                \"Model was initialized without `adata`. The module will be initialized when \"\n",
    "                \"calling `train`. This behavior is experimental and may change in the future.\",\n",
    "                UserWarning,\n",
    "                stacklevel=settings.warnings_stacklevel,\n",
    "            )\n",
    "        else:\n",
    "            # Get categorical covariate info, if available\n",
    "            n_cats_per_cov = (\n",
    "                self.adata_manager.get_state_registry(REGISTRY_KEYS.CAT_COVS_KEY).n_cats_per_key\n",
    "                if REGISTRY_KEYS.CAT_COVS_KEY in self.adata_manager.data_registry\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get number of batches\n",
    "            n_batch = self.summary_stats.n_batch\n",
    "\n",
    "            # Determine if size factor is provided in the data\n",
    "            use_size_factor_key = REGISTRY_KEYS.SIZE_FACTOR_KEY in self.adata_manager.data_registry\n",
    "\n",
    "            # Initialize library size params if needed\n",
    "            library_log_means, library_log_vars = None, None\n",
    "            if (\n",
    "                not use_size_factor_key\n",
    "                and self.minified_data_type != ADATA_MINIFY_TYPE.LATENT_POSTERIOR\n",
    "            ):\n",
    "                library_log_means, library_log_vars = _init_library_size(\n",
    "                    self.adata_manager, n_batch\n",
    "                )\n",
    "\n",
    "            # Instantiate the actual VAE model\n",
    "            self.module = self._module_cls(\n",
    "                n_input=self.summary_stats.n_vars,  # Number of genes\n",
    "                n_batch=n_batch,\n",
    "                n_labels=self.summary_stats.n_labels,\n",
    "                n_continuous_cov=self.summary_stats.get(\"n_extra_continuous_covs\", 0),\n",
    "                n_cats_per_cov=n_cats_per_cov,\n",
    "                n_hidden=n_hidden,\n",
    "                n_latent=n_latent,\n",
    "                n_layers=n_layers,\n",
    "                dropout_rate=dropout_rate,\n",
    "                dispersion=dispersion,\n",
    "                gene_likelihood=gene_likelihood,\n",
    "                latent_distribution=latent_distribution,\n",
    "                use_size_factor_key=use_size_factor_key,\n",
    "                library_log_means=library_log_means,\n",
    "                library_log_vars=library_log_vars,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            # Set minified type to the model (used for memory optimization)\n",
    "            self.module.minified_data_type = self.minified_data_type\n",
    "\n",
    "        # Save init parameters for reproducibility\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "# 5. Define setup_anndata for preproccessing AnnData\n",
    "\n",
    "    @classmethod\n",
    "    @setup_anndata_dsp.dedent  # Automatically formats docstring from template\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        layer: str | None = None,  # Which layer of AnnData.X to use\n",
    "        batch_key: str | None = None,  # Batch annotation column in adata.obs\n",
    "        labels_key: str | None = None,  # Label annotation column\n",
    "        size_factor_key: str | None = None,  # Precomputed size factor\n",
    "        categorical_covariate_keys: list[str] | None = None,  # Categorical covariates\n",
    "        continuous_covariate_keys: list[str] | None = None,   # Continuous covariates\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"%(summary)s.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(param_adata)s\n",
    "        %(param_layer)s\n",
    "        %(param_batch_key)s\n",
    "        %(param_labels_key)s\n",
    "        %(param_size_factor_key)s\n",
    "        %(param_cat_cov_keys)s\n",
    "        %(param_cont_cov_keys)s\n",
    "        \"\"\"\n",
    "\n",
    "        # Get arguments as a dictionary\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "\n",
    "        # Define how to extract relevant fields from AnnData\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, labels_key),\n",
    "            NumericalObsField(REGISTRY_KEYS.SIZE_FACTOR_KEY, size_factor_key, required=False),\n",
    "            CategoricalJointObsField(REGISTRY_KEYS.CAT_COVS_KEY, categorical_covariate_keys),\n",
    "            NumericalJointObsField(REGISTRY_KEYS.CONT_COVS_KEY, continuous_covariate_keys),\n",
    "        ]\n",
    "\n",
    "        # If this is a \"minified\" AnnData, add extra required fields\n",
    "        adata_minify_type = _get_adata_minify_type(adata)\n",
    "        if adata_minify_type is not None:\n",
    "            anndata_fields += cls._get_fields_for_adata_minification(adata_minify_type)\n",
    "\n",
    "        # Create a manager to track and validate all fields\n",
    "        adata_manager = AnnDataManager(fields=anndata_fields, setup_method_args=setup_method_args)\n",
    "\n",
    "        # Register fields into the manager\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "\n",
    "        # Register the manager for this class (global to model)\n",
    "        cls.register_manager(adata_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6ad7f-7d27-4c91-b4e5-9b8fffd7394b",
   "metadata": {},
   "source": [
    "## CC_VAE (inspired by VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa29324-3caf-4f7d-9deb-8075e20e72b7",
   "metadata": {},
   "source": [
    "z ~ q(z | x)\n",
    "α = f(z)\n",
    "c ~ Dirichlet(α)\n",
    "e ~ NB(f(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67fa2b3-4870-4330-ad30-120e6ad2ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.module.base import BaseMinifiedModeModuleClass, EmbeddingModuleMixin, LossOutput, auto_move_data\n",
    "from scvi import REGISTRY_KEYS\n",
    "from scvi.module._constants import MODULE_KEYS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Dirichlet\n",
    "from scvi.distributions import NegativeBinomial\n",
    "\n",
    "\n",
    "class DirichletVAE(EmbeddingModuleMixin, BaseMinifiedModeModuleClass):\n",
    "    \"\"\"\n",
    "    Custom hierarchical VAE:\n",
    "    z ~ N(0, I)\n",
    "    alpha = f(z)\n",
    "    c ~ Dirichlet(alpha)\n",
    "    expression ~ NB(f(c), scale)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        n_latent=10,\n",
    "        n_categories=4,\n",
    "        n_genes=None,\n",
    "        use_observed_lib_size=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_observed_lib_size = use_observed_lib_size\n",
    "\n",
    "        # z encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_input, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU()\n",
    "        )\n",
    "        self.z_mean = nn.Linear(128, n_latent)\n",
    "        self.z_var = nn.Linear(128, n_latent)\n",
    "\n",
    "        # z -> alpha for Dirichlet\n",
    "        self.alpha_decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, 128), nn.ReLU(), nn.Linear(128, n_categories), nn.Softplus()\n",
    "        )\n",
    "\n",
    "        # c -> expression mean\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_categories, 128), nn.ReLU(), nn.Linear(128, n_genes), nn.Softplus()\n",
    "        )\n",
    "\n",
    "        self.px_r = nn.Parameter(torch.randn(n_genes))\n",
    "\n",
    "    @auto_move_data\n",
    "    def forward(self, tensors, compute_loss=True):\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        library = tensors[REGISTRY_KEYS.SIZE_FACTOR_KEY].unsqueeze(-1) if self.use_observed_lib_size else 1.0\n",
    "\n",
    "        h = self.encoder(x)\n",
    "        mu = self.z_mean(h)\n",
    "        var = torch.exp(self.z_var(h)) + 1e-4\n",
    "        qz = Normal(mu, var.sqrt())\n",
    "        z = qz.rsample()\n",
    "\n",
    "        alpha = self.alpha_decoder(z) + 1e-4  # ensure positive alpha\n",
    "        q_dir = Dirichlet(alpha)\n",
    "        c = q_dir.rsample()\n",
    "\n",
    "        px_rate = self.decoder(c) * library\n",
    "        px = NegativeBinomial(mu=px_rate, theta=torch.exp(self.px_r))\n",
    "\n",
    "        if not compute_loss:\n",
    "            return {\"z\": z, \"c\": c, \"dist\": px}\n",
    "\n",
    "        reconst_loss = -px.log_prob(x).sum(-1)\n",
    "        pz = Normal(torch.zeros_like(mu), torch.ones_like(var))\n",
    "        kl_z = torch.distributions.kl_divergence(qz, pz).sum(-1)\n",
    "\n",
    "        loss = torch.mean(reconst_loss + kl_z)\n",
    "\n",
    "        return LossOutput(\n",
    "            loss=loss,\n",
    "            reconstruction_loss=reconst_loss,\n",
    "            kl_local={\"kl_z\": kl_z},\n",
    "            extra_metrics={\"z\": z, \"c\": c},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930deef-6d3d-4f2e-b4c7-41d301b6e853",
   "metadata": {},
   "source": [
    "## MyNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36dbe35d-72ec-4d17-b071-8157652deb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piamozdzanowski/anaconda3/envs/spring_2025/lib/python3.12/site-packages/docrep/decorators.py:43: SyntaxWarning: 'param_categorical_covariate_keys' is not a valid key!\n",
      "  doc = func(self, args[0].__doc__, *args[1:], **kwargs)\n",
      "/Users/piamozdzanowski/anaconda3/envs/spring_2025/lib/python3.12/site-packages/docrep/decorators.py:43: SyntaxWarning: 'param_continuous_covariate_keys' is not a valid key!\n",
      "  doc = func(self, args[0].__doc__, *args[1:], **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Dirichlet\n",
    "from scvi.distributions import NegativeBinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db685041-5295-4637-8a16-a811d3b7d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encode data of ``n_input`` dimensions into a latent space of ``n_output`` dimensions.\n",
    "\n",
    "    Uses a fully-connected neural network of ``n_hidden`` layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        The dimensionality of the input (data space)\n",
    "    n_output\n",
    "        The dimensionality of the output (latent space)\n",
    "    n_cat_list\n",
    "        A list containing the number of categories\n",
    "        for each category of interest. Each category will be\n",
    "        included using a one-hot encoding\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    distribution\n",
    "        Distribution of z\n",
    "    var_eps\n",
    "        Minimum value for the variance;\n",
    "        used for numerical stability\n",
    "    var_activation\n",
    "        Callable used to ensure positivity of the variance.\n",
    "        Defaults to :meth:`torch.exp`.\n",
    "    return_dist\n",
    "        Return directly the distribution of z instead of its parameters.\n",
    "    **kwargs\n",
    "        Keyword args for :class:`~scvi.nn.FCLayers`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        dropout_rate: float = 0.1,\n",
    "        distribution: str = \"normal\",\n",
    "        var_eps: float = 1e-4,\n",
    "        var_activation: Callable | None = None,\n",
    "        return_dist: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.distribution = distribution\n",
    "        self.var_eps = var_eps\n",
    "        self.encoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.mean_encoder = nn.Linear(n_hidden, n_output)\n",
    "        self.var_encoder = nn.Linear(n_hidden, n_output)\n",
    "        self.return_dist = return_dist\n",
    "\n",
    "        if distribution == \"ln\":\n",
    "            self.z_transformation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            self.z_transformation = _identity\n",
    "        self.var_activation = torch.exp if var_activation is None else var_activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *cat_list: int):\n",
    "        r\"\"\"The forward computation for a single sample.\n",
    "\n",
    "         #. Encodes the data into latent space using the encoder network\n",
    "         #. Generates a mean \\\\( q_m \\\\) and variance \\\\( q_v \\\\)\n",
    "         #. Samples a new value from an i.i.d. multivariate normal\n",
    "            \\\\( \\\\sim Ne(q_m, \\\\mathbf{I}q_v) \\\\)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor with shape (n_input,)\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        3-tuple of :py:class:`torch.Tensor`\n",
    "            tensors of shape ``(n_latent,)`` for mean and var, and sample\n",
    "\n",
    "        \"\"\"\n",
    "        # Parameters for latent distribution\n",
    "        q = self.encoder(x, *cat_list)\n",
    "        q_m = self.mean_encoder(q)\n",
    "        q_v = self.var_activation(self.var_encoder(q)) + self.var_eps\n",
    "        dist = Normal(q_m, q_v.sqrt())\n",
    "        latent = self.z_transformation(dist.rsample())\n",
    "        if self.return_dist:\n",
    "            return dist, latent\n",
    "        return q_m, q_v, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c43592-a782-434b-9af2-435a95c63164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletDecoder(nn.Module):\n",
    "    def __init__(self, n_latent, n_categories, n_hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_categories),\n",
    "            nn.Softplus()  # ensure alpha > 0\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        alpha = self.net(z) + 1e-4\n",
    "        return Dirichlet(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6a5b80-172b-4ba7-9b6a-0283ef2ffc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionDecoder(nn.Module):\n",
    "    def __init__(self, n_categories, n_genes, n_hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_categories, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_genes),\n",
    "            nn.Softplus()  # mean must be positive\n",
    "        )\n",
    "        self.px_r = nn.Parameter(torch.randn(n_genes))  # gene-specific dispersion\n",
    "\n",
    "    def forward(self, c, library_size):\n",
    "        mean = self.net(c)\n",
    "        scaled_mean = mean * library_size.unsqueeze(-1)\n",
    "        return NegativeBinomial(mu=scaled_mean, theta=torch.exp(self.px_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33dbedbe-ca29-4cde-a921-1a4cba84c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHierarchicalVAE(nn.Module):\n",
    "    def __init__(self, n_input, n_latent, n_categories, n_genes):\n",
    "        super().__init__()\n",
    "        self.encoder_z = EncoderZ(n_input, n_latent)\n",
    "        self.decoder_dirichlet = DirichletDecoder(n_latent, n_categories)\n",
    "        self.decoder_expression = ExpressionDecoder(n_categories, n_genes)\n",
    "\n",
    "    def forward(self, x, library_size):\n",
    "        qz = self.encoder_z(x)\n",
    "        z = qz.rsample()\n",
    "        q_dir = self.decoder_dirichlet(z)\n",
    "        c = q_dir.rsample()\n",
    "        px = self.decoder_expression(c, library_size)\n",
    "        return qz, q_dir, px, z, c\n",
    "\n",
    "    def loss(self, x, library_size):\n",
    "        qz, q_dir, px, z, c = self.forward(x, library_size)\n",
    "        # Negative log-likelihood\n",
    "        recon_loss = -px.log_prob(x).sum(-1)\n",
    "        # KL[q(z|x) || p(z)]\n",
    "        pz = Normal(torch.zeros_like(z), torch.ones_like(z))\n",
    "        kl_z = torch.distributions.kl_divergence(qz, pz).sum(-1)\n",
    "        # No KL for c because it's from p(c|z)\n",
    "        total_loss = (recon_loss + kl_z).mean()\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62d2bda2-cb94-4b62-b8ce-5be524e9e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume `x` is (n_cells, n_genes)\n",
    "x = torch.randn(1000, 1000)\n",
    "lib_size = x.sum(dim=1)\n",
    "\n",
    "dataset = TensorDataset(x, lib_size)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "model = MyHierarchicalVAE(n_input=1000, n_latent=10, n_categories=4, n_genes=1000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaaee0d7-2454-4878-9203-f7d17bf34c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyHierarchicalVAE(\n",
       "  (encoder_z): EncoderZ(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=1000, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (mean_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (var_encoder): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (decoder_dirichlet): DirichletDecoder(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=4, bias=True)\n",
       "      (3): Softplus(beta=1.0, threshold=20.0)\n",
       "    )\n",
       "  )\n",
       "  (decoder_expression): ExpressionDecoder(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=1000, bias=True)\n",
       "      (3): Softplus(beta=1.0, threshold=20.0)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 400\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895af159-53f8-4617-9057-759aaf03e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Dirichlet\n",
    "from typing import Iterable, Literal\n",
    "\n",
    "\n",
    "class DecoderCCVI(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,           # latent dimension\n",
    "        n_output: int,          # number of genes\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        inject_covariates: bool = True,\n",
    "        use_batch_norm: bool = False,\n",
    "        use_layer_norm: bool = False,\n",
    "        scale_activation: Literal[\"softmax\", \"softplus\"] = \"softmax\",\n",
    "        dirichlet_dim: int = 4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # z → Dirichlet parameters (α1, ..., α4)\n",
    "        self.dirichlet_param_net = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=dirichlet_dim,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=0,\n",
    "            inject_covariates=inject_covariates,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.dirichlet_activation = nn.Softplus()  # ensure positive α values\n",
    "\n",
    "        # c → px_scale (expression proportions)\n",
    "        if scale_activation == \"softmax\":\n",
    "            px_scale_activation = nn.Softmax(dim=-1)\n",
    "        elif scale_activation == \"softplus\":\n",
    "            px_scale_activation = nn.Softplus()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation\")\n",
    "\n",
    "        self.cell_cycle_decoder = nn.Sequential(\n",
    "            nn.Linear(dirichlet_dim, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output),\n",
    "            px_scale_activation,\n",
    "        )\n",
    "\n",
    "        self.px_r_decoder = nn.Linear(n_output, n_output)  # Optional: gene-cell dispersion\n",
    "        #self.px_dropout_decoder = nn.Linear(n_output, n_output)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        dispersion: str,\n",
    "        z: torch.Tensor,\n",
    "        library: torch.Tensor,\n",
    "        *cat_list: int,\n",
    "    ):\n",
    "        # 1. Generate Dirichlet parameters from z\n",
    "        a = self.dirichlet_param_net(z, *cat_list)\n",
    "        a = self.dirichlet_activation(a) + 1e-4  # ensure positivity\n",
    "\n",
    "        # 2. Sample cell cycle phase from Dirichlet\n",
    "        dirichlet_dist = Dirichlet(a)\n",
    "        c = dirichlet_dist.rsample()  # reparameterized sample for backprop\n",
    "\n",
    "        # 3. Generate px_scale from c\n",
    "        px_scale = self.cell_cycle_decoder(c)\n",
    "\n",
    "        # 4. Dropout and rate\n",
    "        px_dropout = None # self.px_dropout_decoder(px_scale)\n",
    "        px_rate = torch.exp(library) * px_scale\n",
    "        px_r = self.px_r_decoder(px_scale) if dispersion == \"gene-cell\" else None\n",
    "\n",
    "        return px_scale, px_r, px_rate, px_dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa2271-6210-4973-8281-8b9145ae3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CC_VAE(\n",
    "    EmbeddingModuleMixin, BaseMinifiedModeModuleClass\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        n_batch=0,\n",
    "        n_labels=0,\n",
    "        n_hidden=128,\n",
    "        n_latent=10,\n",
    "        n_layers=1,\n",
    "        n_continuous_cov=0,\n",
    "        n_cats_per_cov=None,\n",
    "        dropout_rate=0.1,\n",
    "        dispersion=\"gene\",\n",
    "        log_variational=True,\n",
    "        gene_likelihood=\"nb\",\n",
    "        latent_distribution=\"normal\",\n",
    "        encode_covariates=False,\n",
    "        deeply_inject_covariates=True,\n",
    "        batch_representation=\"one-hot\",\n",
    "        use_batch_norm=\"both\",\n",
    "        use_layer_norm=\"none\",\n",
    "        use_size_factor_key=False,\n",
    "        use_observed_lib_size=True,\n",
    "        library_log_means=None,\n",
    "        library_log_vars=None,\n",
    "        var_activation=None,\n",
    "        extra_encoder_kwargs=None,\n",
    "        extra_decoder_kwargs=None,\n",
    "        batch_embedding_kwargs=None,\n",
    "    ):\n",
    "        from scvi.nn import DecoderSCVI, Encoder\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dispersion = dispersion\n",
    "        self.n_latent = n_latent\n",
    "        self.log_variational = log_variational\n",
    "        self.gene_likelihood = gene_likelihood\n",
    "        self.n_batch = n_batch\n",
    "        self.n_labels = n_labels\n",
    "        self.latent_distribution = latent_distribution\n",
    "        self.encode_covariates = encode_covariates\n",
    "        self.use_size_factor_key = use_size_factor_key\n",
    "        self.use_observed_lib_size = use_size_factor_key or use_observed_lib_size\n",
    "\n",
    "        if not self.use_observed_lib_size:\n",
    "            if library_log_means is None or library_log_vars is None:\n",
    "                raise ValueError(\"Must provide library_log_means and library_log_vars if not using observed_lib_size.\")\n",
    "            self.register_buffer(\"library_log_means\", torch.from_numpy(library_log_means).float())\n",
    "            self.register_buffer(\"library_log_vars\", torch.from_numpy(library_log_vars).float())\n",
    "\n",
    "        if self.dispersion == \"gene\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input))\n",
    "        elif self.dispersion == \"gene-batch\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input, n_batch))\n",
    "        elif self.dispersion == \"gene-label\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input, n_labels))\n",
    "        elif self.dispersion != \"gene-cell\":\n",
    "            raise ValueError(\"`dispersion` must be one of 'gene', 'gene-batch', 'gene-label', 'gene-cell'.\")\n",
    "\n",
    "        self.batch_representation = batch_representation\n",
    "        if self.batch_representation == \"embedding\":\n",
    "            self.init_embedding(REGISTRY_KEYS.BATCH_KEY, n_batch, **(batch_embedding_kwargs or {}))\n",
    "            batch_dim = self.get_embedding(REGISTRY_KEYS.BATCH_KEY).embedding_dim\n",
    "        elif self.batch_representation != \"one-hot\":\n",
    "            raise ValueError(\"`batch_representation` must be one of 'one-hot', 'embedding'.\")\n",
    "\n",
    "        use_batch_norm_encoder = use_batch_norm in [\"encoder\", \"both\"]\n",
    "        use_batch_norm_decoder = use_batch_norm in [\"decoder\", \"both\"]\n",
    "        use_layer_norm_encoder = use_layer_norm in [\"encoder\", \"both\"]\n",
    "        use_layer_norm_decoder = use_layer_norm in [\"decoder\", \"both\"]\n",
    "\n",
    "        n_input_encoder = n_input + n_continuous_cov * encode_covariates\n",
    "        if self.batch_representation == \"embedding\":\n",
    "            n_input_encoder += batch_dim * encode_covariates\n",
    "            cat_list = list([] if n_cats_per_cov is None else n_cats_per_cov)\n",
    "        else:\n",
    "            cat_list = [n_batch] + list([] if n_cats_per_cov is None else n_cats_per_cov)\n",
    "\n",
    "        encoder_cat_list = cat_list if encode_covariates else None\n",
    "        _extra_encoder_kwargs = extra_encoder_kwargs or {}\n",
    "\n",
    "        self.z_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            n_latent,\n",
    "            n_cat_list=encoder_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            distribution=latent_distribution,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "            var_activation=var_activation,\n",
    "            return_dist=True,\n",
    "            **_extra_encoder_kwargs,\n",
    "        )\n",
    "\n",
    "        self.l_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            1,\n",
    "            n_layers=1,\n",
    "            n_cat_list=encoder_cat_list,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "            var_activation=var_activation,\n",
    "            return_dist=True,\n",
    "            **_extra_encoder_kwargs,\n",
    "        )\n",
    "\n",
    "        n_input_decoder = n_latent + n_continuous_cov\n",
    "        if self.batch_representation == \"embedding\":\n",
    "            n_input_decoder += batch_dim\n",
    "\n",
    "        _extra_decoder_kwargs = extra_decoder_kwargs or {}\n",
    "        self.decoder = DecoderSCVI(\n",
    "            n_input_decoder,\n",
    "            n_input,\n",
    "            n_cat_list=cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_decoder,\n",
    "            use_layer_norm=use_layer_norm_decoder,\n",
    "            scale_activation=\"softplus\" if use_size_factor_key else \"softmax\",\n",
    "            dirichlet_dim=4,\n",
    "            **_extra_decoder_kwargs,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
