{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cd9943-d00b-4c4e-8abd-b3ec763098db",
   "metadata": {},
   "source": [
    "# VAE Annotated\n",
    "This is the core variational autoencoder (VAE) used by the scVI framework for modeling scRNA-seq data. It encodes gene expression into a latent space and reconstructs it via probabilistic decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13139280-d5ec-466b-b6b3-2b49e3935c91",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d21ec-0fa0-419f-8258-ff0d02453618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # Enable postponed evaluation of annotations (so type hints can refer to classes defined later)\n",
    "\n",
    "import logging                    # For logging messages\n",
    "import warnings                   # To issue warning messages\n",
    "from typing import TYPE_CHECKING  # For type-checking-only imports\n",
    "\n",
    "import numpy as np              # Numerical computing library\n",
    "import torch                    # PyTorch library for tensor computations and neural networks\n",
    "from torch.nn.functional import one_hot  # Function to convert indices to one-hot encoded tensors\n",
    "\n",
    "# Import constants and settings from the scvi package\n",
    "from scvi import REGISTRY_KEYS, settings\n",
    "from scvi.data._constants import ADATA_MINIFY_TYPE  # Constants for different AnnData minification types\n",
    "from scvi.module._constants import MODULE_KEYS      # Keys used for module input/output\n",
    "# Import base classes and mixins for modules\n",
    "from scvi.module.base import (\n",
    "    BaseMinifiedModeModuleClass,  # Base class for modules that support minified AnnData modes\n",
    "    EmbeddingModuleMixin,         # Mixin to add embedding-related functionalities\n",
    "    LossOutput,                   # Structure for returning loss and metrics\n",
    "    auto_move_data,               # Decorator to automatically move data to the proper device\n",
    ")\n",
    "from scvi.utils import unsupported_if_adata_minified  # Decorator to mark methods unsupported for minified data\n",
    "\n",
    "# Import additional types only during type checking to avoid circular imports\n",
    "if TYPE_CHECKING:\n",
    "    from collections.abc import Callable  # Type for callable objects\n",
    "    from typing import Literal            # Literal type for specific string values\n",
    "    from torch.distributions import Distribution  # For representing probability distributions\n",
    "\n",
    "# Create a logger for this module using the module's name\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f69111-b19d-498d-a677-4846bc071fad",
   "metadata": {},
   "source": [
    "## Create VAE Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b04fed-bc63-46f4-833f-cce366072094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(\n",
    "# 1. CLASS INHERITANCE\n",
    "    EmbeddingModuleMixin, BaseMinifiedModeModuleClass):\n",
    "# 2. ClASS DOCSTRING\n",
    "    \"\"\"\n",
    "    Variational auto-encoder :cite:p:`Lopez18`.\n",
    "\n",
    "    This class implements a variational autoencoder (VAE) for single-cell RNA-seq data.\n",
    "    It inherits from an embedding mixin (for latent representations) and a base module class\n",
    "    that supports minified AnnData mode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input features.\n",
    "    n_batch\n",
    "        Number of batches. If ``0``, no batch correction is performed.\n",
    "    n_labels\n",
    "        Number of labels.\n",
    "    n_hidden\n",
    "        Number of nodes per hidden layer. Passed into Encoder and DecoderSCVI.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    n_layers\n",
    "        Number of hidden layers. Passed into Encoder and DecoderSCVI.\n",
    "    n_continuous_cov\n",
    "        Number of continuous covariates.\n",
    "    n_cats_per_cov\n",
    "        A list of integers containing the number of categories for each categorical covariate.\n",
    "    dropout_rate\n",
    "        Dropout rate, passed into the Encoder.\n",
    "    dispersion\n",
    "        Parameter controlling dispersion for the likelihood distribution.\n",
    "    log_variational\n",
    "        Whether to apply log1p to input data for numerical stability.\n",
    "    gene_likelihood\n",
    "        Likelihood distribution for gene expression (e.g. \"zinb\", \"nb\", \"poisson\").\n",
    "    latent_distribution\n",
    "        Distribution for the latent space (e.g. \"normal\", \"ln\").\n",
    "    encode_covariates\n",
    "        Whether to concatenate covariates to the gene expression before encoding.\n",
    "    deeply_inject_covariates\n",
    "        If True and n_layers > 1, covariates are injected at each hidden layer.\n",
    "    batch_representation\n",
    "        How to represent batch information (\"one-hot\" or \"embedding\").\n",
    "    use_batch_norm\n",
    "        Where to use Batch Normalization (\"none\", \"encoder\", \"decoder\", \"both\").\n",
    "    use_layer_norm\n",
    "        Where to use Layer Normalization (\"none\", \"encoder\", \"decoder\", \"both\").\n",
    "    use_size_factor_key\n",
    "        If True, use an AnnData.obs column as the scaling factor for the likelihood.\n",
    "    use_observed_lib_size\n",
    "        If True, use the observed library size for scaling.\n",
    "    library_log_means\n",
    "        Numpy array with means for log library sizes (if not using observed library size).\n",
    "    library_log_vars\n",
    "        Numpy array with variances for log library sizes (if not using observed library size).\n",
    "    var_activation\n",
    "        Callable for ensuring positivity of the variance output in the encoder.\n",
    "    extra_encoder_kwargs\n",
    "        Extra keyword arguments for the Encoder.\n",
    "    extra_decoder_kwargs\n",
    "        Extra keyword arguments for the DecoderSCVI.\n",
    "    batch_embedding_kwargs\n",
    "        Keyword arguments for the batch embedding layer (if using embedding representation).\n",
    "    \"\"\"\n",
    "\n",
    "# 3. CONSTRUCTOR\n",
    "    '''\n",
    "    - Checks and stores all parameters\n",
    "    - Handles dispersion type (per gene/cell/batch/label)\n",
    "    - Initializes encoders: \n",
    "            z_encoder for latent variables\n",
    "            l_encoder for library size\n",
    "    - Initializes decoder based on latent space + covariates\n",
    "    - Handles categorical and continuous covariates'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,   # Number of input features (e.g., number of genes)\n",
    "        n_batch: int = 0,  # Number of batches; 0 implies no batch correction\n",
    "        n_labels: int = 0,  # Number of label classes (if any)\n",
    "        n_hidden: int = 128,  # Number of nodes in each hidden layer\n",
    "        n_latent: int = 10,   # Dimensionality of the latent space\n",
    "        n_layers: int = 1,    # Number of layers in the encoder/decoder networks\n",
    "        n_continuous_cov: int = 0,  # Number of continuous covariates\n",
    "        n_cats_per_cov: list[int] | None = None,  # List with number of categories for each categorical covariate\n",
    "        dropout_rate: float = 0.1,  # Dropout rate for the neural network layers\n",
    "        dispersion: Literal[\"gene\", \"gene-batch\", \"gene-label\", \"gene-cell\"] = \"gene\",\n",
    "            # Dispersion model: how variance is modeled (per gene, per batch, etc.)\n",
    "        log_variational: bool = True,  # Whether to apply log1p on input data for numerical stability\n",
    "        gene_likelihood: Literal[\"zinb\", \"nb\", \"poisson\"] = \"zinb\",\n",
    "            # Likelihood model for gene expression (Zero-Inflated Negative Binomial, etc.)\n",
    "        latent_distribution: Literal[\"normal\", \"ln\"] = \"normal\",\n",
    "            # Distribution used for latent variables (\"normal\" or \"logistic normal\")\n",
    "        encode_covariates: bool = False,  # Whether to concatenate covariates with gene expression data\n",
    "        deeply_inject_covariates: bool = True,  # Whether to inject covariates at deeper layers in the encoder/decoder\n",
    "        batch_representation: Literal[\"one-hot\", \"embedding\"] = \"one-hot\",\n",
    "            # How to represent batch information (one-hot vector or learned embedding)\n",
    "        use_batch_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"both\",\n",
    "            # Where to apply Batch Normalization in the network\n",
    "        use_layer_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"none\",\n",
    "            # Where to apply Layer Normalization in the network\n",
    "        use_size_factor_key: bool = False,  # Whether to use size factors from AnnData.obs as scaling factors\n",
    "        use_observed_lib_size: bool = True,  # Whether to use the observed library size directly for scaling\n",
    "        library_log_means: np.ndarray | None = None,  # Precomputed means of log library sizes (if not observed)\n",
    "        library_log_vars: np.ndarray | None = None,   # Precomputed variances of log library sizes (if not observed)\n",
    "        var_activation: Callable[[torch.Tensor], torch.Tensor] = None,  # Activation for variance output (e.g., torch.exp)\n",
    "        extra_encoder_kwargs: dict | None = None,  # Additional parameters for the Encoder\n",
    "        extra_decoder_kwargs: dict | None = None,  # Additional parameters for the DecoderSCVI\n",
    "        batch_embedding_kwargs: dict | None = None,  # Additional parameters for batch embedding layer (if used)\n",
    "    ):\n",
    "        from scvi.nn import DecoderSCVI, Encoder  # Import Encoder and Decoder classes from scvi.nn\n",
    "\n",
    "        super().__init__()  # Initialize parent classes (EmbeddingModuleMixin and BaseMinifiedModeModuleClass)\n",
    "\n",
    "        # Store various model parameters as attributes\n",
    "        self.dispersion = dispersion\n",
    "        self.n_latent = n_latent\n",
    "        self.log_variational = log_variational\n",
    "        self.gene_likelihood = gene_likelihood\n",
    "        self.n_batch = n_batch\n",
    "        self.n_labels = n_labels\n",
    "        self.latent_distribution = latent_distribution\n",
    "        self.encode_covariates = encode_covariates\n",
    "        self.use_size_factor_key = use_size_factor_key\n",
    "        # If size factor key is used, then use_observed_lib_size is True; otherwise, use the provided flag\n",
    "        self.use_observed_lib_size = use_size_factor_key or use_observed_lib_size\n",
    "\n",
    "        # If not using observed library size, then library_log_means and library_log_vars must be provided\n",
    "        if not self.use_observed_lib_size:\n",
    "            if library_log_means is None or library_log_vars is None:\n",
    "                raise ValueError(\n",
    "                    \"If not using observed_lib_size, must provide library_log_means and library_log_vars.\"\n",
    "                )\n",
    "            # Register these as buffers (non-parameter tensors that move with the model)\n",
    "            self.register_buffer(\"library_log_means\", torch.from_numpy(library_log_means).float())\n",
    "            self.register_buffer(\"library_log_vars\", torch.from_numpy(library_log_vars).float())\n",
    "\n",
    "        # Initialize dispersion parameter(s) based on the chosen model\n",
    "        if self.dispersion == \"gene\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input))  # One parameter per gene\n",
    "        elif self.dispersion == \"gene-batch\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input, n_batch))  # Parameter per gene for each batch\n",
    "        elif self.dispersion == \"gene-label\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input, n_labels))  # Parameter per gene for each label\n",
    "        elif self.dispersion == \"gene-cell\":\n",
    "            # For \"gene-cell\", parameters are modeled differently (typically learned per cell during training)\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`dispersion` must be one of 'gene', 'gene-batch', 'gene-label', 'gene-cell'.\"\n",
    "            )\n",
    "\n",
    "        # Setup batch representation; if embedding is chosen, initialize an embedding layer\n",
    "        self.batch_representation = batch_representation\n",
    "        if self.batch_representation == \"embedding\":\n",
    "            # Initialize embedding for batches using a key from REGISTRY_KEYS and extra kwargs if provided\n",
    "            self.init_embedding(REGISTRY_KEYS.BATCH_KEY, n_batch, **(batch_embedding_kwargs or {}))\n",
    "            # Get the embedding dimension from the initialized embedding layer\n",
    "            batch_dim = self.get_embedding(REGISTRY_KEYS.BATCH_KEY).embedding_dim\n",
    "        elif self.batch_representation != \"one-hot\":\n",
    "            raise ValueError(\"`batch_representation` must be one of 'one-hot', 'embedding'.\")\n",
    "\n",
    "        # Determine where to apply normalization in the encoder and decoder\n",
    "        use_batch_norm_encoder = use_batch_norm == \"encoder\" or use_batch_norm == \"both\"\n",
    "        use_batch_norm_decoder = use_batch_norm == \"decoder\" or use_batch_norm == \"both\"\n",
    "        use_layer_norm_encoder = use_layer_norm == \"encoder\" or use_layer_norm == \"both\"\n",
    "        use_layer_norm_decoder = use_layer_norm == \"decoder\" or use_layer_norm == \"both\"\n",
    "\n",
    "        # Calculate the input dimension for the encoder. Start with gene counts and add covariates if encoding them.\n",
    "        n_input_encoder = n_input + n_continuous_cov * encode_covariates\n",
    "        if self.batch_representation == \"embedding\":\n",
    "            n_input_encoder += batch_dim * encode_covariates  # Add batch embedding dimension if applicable\n",
    "            cat_list = list([] if n_cats_per_cov is None else n_cats_per_cov)\n",
    "        else:\n",
    "            # For one-hot, add n_batch as a categorical variable\n",
    "            cat_list = [n_batch] + list([] if n_cats_per_cov is None else n_cats_per_cov)\n",
    "\n",
    "        # Only include categorical covariates if requested\n",
    "        encoder_cat_list = cat_list if encode_covariates else None\n",
    "        _extra_encoder_kwargs = extra_encoder_kwargs or {}\n",
    "        # Initialize the encoder for the latent variable \"z\"\n",
    "        self.z_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            n_latent,\n",
    "            n_cat_list=encoder_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            distribution=latent_distribution,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "            var_activation=var_activation,\n",
    "            return_dist=True,  # Return a distribution rather than a fixed tensor\n",
    "            **_extra_encoder_kwargs,\n",
    "        )\n",
    "        # Initialize a separate encoder for the library size \"l\" (1-dimensional)\n",
    "        self.l_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            1,\n",
    "            n_layers=1,\n",
    "            n_cat_list=encoder_cat_list,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "            var_activation=var_activation,\n",
    "            return_dist=True,\n",
    "            **_extra_encoder_kwargs,\n",
    "        )\n",
    "        # Calculate input dimension for the decoder: latent dimension plus continuous covariates\n",
    "        n_input_decoder = n_latent + n_continuous_cov\n",
    "        if self.batch_representation == \"embedding\":\n",
    "            n_input_decoder += batch_dim  # Add embedding dimension for batch representation\n",
    "\n",
    "        _extra_decoder_kwargs = extra_decoder_kwargs or {}\n",
    "        # Initialize the decoder module that maps latent space back to the original input space\n",
    "        self.decoder = DecoderSCVI(\n",
    "            n_input_decoder,\n",
    "            n_input,\n",
    "            n_cat_list=cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_decoder,\n",
    "            use_layer_norm=use_layer_norm_decoder,\n",
    "            scale_activation=\"softplus\" if use_size_factor_key else \"softmax\",\n",
    "                # Activation for scaling: depends on whether size factor key is used\n",
    "            **_extra_decoder_kwargs,\n",
    "        )\n",
    "\n",
    "# 4. Prepare tensors for inference \n",
    "    def _get_inference_input(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor | None],\n",
    "        full_forward_pass: bool = False,\n",
    "    ) -> dict[str, torch.Tensor | None]:\n",
    "        \"\"\"Get input tensors for the inference process.\"\"\"\n",
    "        # Decide which data loader to use based on full_forward_pass flag and the minified data type\n",
    "        if full_forward_pass or self.minified_data_type is None:\n",
    "            loader = \"full_data\"\n",
    "        elif self.minified_data_type in [\n",
    "            ADATA_MINIFY_TYPE.LATENT_POSTERIOR,\n",
    "            ADATA_MINIFY_TYPE.LATENT_POSTERIOR_WITH_COUNTS,\n",
    "        ]:\n",
    "            loader = \"minified_data\"\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown minified-data type: {self.minified_data_type}\")\n",
    "\n",
    "        # For full data, return the standard tensors used in the model\n",
    "        if loader == \"full_data\":\n",
    "            return {\n",
    "                MODULE_KEYS.X_KEY: tensors[REGISTRY_KEYS.X_KEY],\n",
    "                MODULE_KEYS.BATCH_INDEX_KEY: tensors[REGISTRY_KEYS.BATCH_KEY],\n",
    "                MODULE_KEYS.CONT_COVS_KEY: tensors.get(REGISTRY_KEYS.CONT_COVS_KEY, None),\n",
    "                MODULE_KEYS.CAT_COVS_KEY: tensors.get(REGISTRY_KEYS.CAT_COVS_KEY, None),\n",
    "            }\n",
    "        else:\n",
    "            # For minified data, use cached latent parameters\n",
    "            return {\n",
    "                MODULE_KEYS.QZM_KEY: tensors[REGISTRY_KEYS.LATENT_QZM_KEY],\n",
    "                MODULE_KEYS.QZV_KEY: tensors[REGISTRY_KEYS.LATENT_QZV_KEY],\n",
    "                REGISTRY_KEYS.OBSERVED_LIB_SIZE: tensors[REGISTRY_KEYS.OBSERVED_LIB_SIZE],\n",
    "            }\n",
    "\n",
    "# 5. Prepare tensors for generative model  \n",
    "    def _get_generative_input(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],\n",
    "        inference_outputs: dict[str, torch.Tensor | Distribution | None],\n",
    "    ) -> dict[str, torch.Tensor | None]:\n",
    "        \"\"\"Get input tensors for the generative process.\"\"\"\n",
    "        # Retrieve and transform size factor if provided\n",
    "        size_factor = tensors.get(REGISTRY_KEYS.SIZE_FACTOR_KEY, None)\n",
    "        if size_factor is not None:\n",
    "            size_factor = torch.log(size_factor)\n",
    "\n",
    "        # Return a dictionary mapping module keys to the appropriate tensors/distributions\n",
    "        return {\n",
    "            MODULE_KEYS.Z_KEY: inference_outputs[MODULE_KEYS.Z_KEY],\n",
    "            MODULE_KEYS.LIBRARY_KEY: inference_outputs[MODULE_KEYS.LIBRARY_KEY],\n",
    "            MODULE_KEYS.BATCH_INDEX_KEY: tensors[REGISTRY_KEYS.BATCH_KEY],\n",
    "            MODULE_KEYS.Y_KEY: tensors[REGISTRY_KEYS.LABELS_KEY],\n",
    "            MODULE_KEYS.CONT_COVS_KEY: tensors.get(REGISTRY_KEYS.CONT_COVS_KEY, None),\n",
    "            MODULE_KEYS.CAT_COVS_KEY: tensors.get(REGISTRY_KEYS.CAT_COVS_KEY, None),\n",
    "            MODULE_KEYS.SIZE_FACTOR_KEY: size_factor,\n",
    "        }\n",
    "\n",
    "# 6. For each cell, computes the mean and variance of the log library size for the corresponding batch.\n",
    "    def _compute_local_library_params(\n",
    "        self,\n",
    "        batch_index: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes local library parameters.\n",
    "\n",
    "        For each cell, computes the mean and variance of the log library size\n",
    "        for the corresponding batch.\n",
    "        \"\"\"\n",
    "        from torch.nn.functional import linear\n",
    "\n",
    "        n_batch = self.library_log_means.shape[1]  # Number of batches from the library means buffer\n",
    "        # Compute local means using one-hot encoding for the batch index and linear transformation\n",
    "        local_library_log_means = linear(\n",
    "            one_hot(batch_index.squeeze(-1), n_batch).float(), self.library_log_means\n",
    "        )\n",
    "        # Compute local variances similarly\n",
    "        local_library_log_vars = linear(\n",
    "            one_hot(batch_index.squeeze(-1), n_batch).float(), self.library_log_vars\n",
    "        )\n",
    "\n",
    "        return local_library_log_means, local_library_log_vars\n",
    "\n",
    "    @auto_move_data  # Automatically move inputs/outputs to the correct device (CPU/GPU)\n",
    "\n",
    "# 7. Encodes input data into latent variables:\n",
    "    def _regular_inference(\n",
    "        self,\n",
    "        x: torch.Tensor,                 # Input gene expression data\n",
    "        batch_index: torch.Tensor,       # Batch indices for each cell\n",
    "        cont_covs: torch.Tensor | None = None,  # Continuous covariates (if any)\n",
    "        cat_covs: torch.Tensor | None = None,   # Categorical covariates (if any)\n",
    "        n_samples: int = 1,                # Number of samples for Monte Carlo approximation\n",
    "    ) -> dict[str, torch.Tensor | Distribution | None]:\n",
    "        \"\"\"Run the regular inference process.\"\"\"\n",
    "        x_ = x  # Make a copy of the input\n",
    "        # If using observed library size, compute it as the log sum of gene counts\n",
    "        if self.use_observed_lib_size:\n",
    "            library = torch.log(x.sum(1)).unsqueeze(1)\n",
    "        # Apply logarithmic transformation if specified\n",
    "        if self.log_variational:\n",
    "            x_ = torch.log1p(x_)\n",
    "\n",
    "        # Concatenate continuous covariates if they are to be encoded\n",
    "        if cont_covs is not None and self.encode_covariates:\n",
    "            encoder_input = torch.cat((x_, cont_covs), dim=-1)\n",
    "        else:\n",
    "            encoder_input = x_\n",
    "        # Split categorical covariates along the feature dimension if available\n",
    "        if cat_covs is not None and self.encode_covariates:\n",
    "            categorical_input = torch.split(cat_covs, 1, dim=1)\n",
    "        else:\n",
    "            categorical_input = ()\n",
    "\n",
    "        # If using an embedding for batch information, compute and concatenate it to the encoder input\n",
    "        if self.batch_representation == \"embedding\" and self.encode_covariates:\n",
    "            batch_rep = self.compute_embedding(REGISTRY_KEYS.BATCH_KEY, batch_index)\n",
    "            encoder_input = torch.cat([encoder_input, batch_rep], dim=-1)\n",
    "            qz, z = self.z_encoder(encoder_input, *categorical_input)\n",
    "        else:\n",
    "            # Otherwise, pass batch_index directly to the encoder\n",
    "            qz, z = self.z_encoder(encoder_input, batch_index, *categorical_input)\n",
    "\n",
    "        ql = None\n",
    "        # If not using observed library size, encode library size using l_encoder\n",
    "        if not self.use_observed_lib_size:\n",
    "            if self.batch_representation == \"embedding\":\n",
    "                ql, library_encoded = self.l_encoder(encoder_input, *categorical_input)\n",
    "            else:\n",
    "                ql, library_encoded = self.l_encoder(encoder_input, batch_index, *categorical_input)\n",
    "            library = library_encoded\n",
    "\n",
    "        # If more than one Monte Carlo sample is requested, sample from the distributions\n",
    "        if n_samples > 1:\n",
    "            untran_z = qz.sample((n_samples,))\n",
    "            z = self.z_encoder.z_transformation(untran_z)\n",
    "            if self.use_observed_lib_size:\n",
    "                library = library.unsqueeze(0).expand(\n",
    "                    (n_samples, library.size(0), library.size(1))\n",
    "                )\n",
    "            else:\n",
    "                library = ql.sample((n_samples,))\n",
    "\n",
    "        # Return the inference outputs as a dictionary\n",
    "        return {\n",
    "            MODULE_KEYS.Z_KEY: z,\n",
    "            MODULE_KEYS.QZ_KEY: qz,\n",
    "            MODULE_KEYS.QL_KEY: ql,\n",
    "            MODULE_KEYS.LIBRARY_KEY: library,\n",
    "        }\n",
    "\n",
    "    @auto_move_data\n",
    "    def _cached_inference(\n",
    "        self,\n",
    "        qzm: torch.Tensor,          # Cached latent mean values\n",
    "        qzv: torch.Tensor,          # Cached latent variance values\n",
    "        observed_lib_size: torch.Tensor,  # Observed library size values\n",
    "        n_samples: int = 1,         # Number of samples for Monte Carlo approximation\n",
    "    ) -> dict[str, torch.Tensor | None]:\n",
    "        \"\"\"Run the cached inference process.\"\"\"\n",
    "        from torch.distributions import Normal\n",
    "\n",
    "        # Reconstruct the latent distribution using the cached parameters\n",
    "        qz = Normal(qzm, qzv.sqrt())\n",
    "        # Sample from the latent distribution; using sample() (non-reparameterized)\n",
    "        untran_z = qz.sample() if n_samples == 1 else qz.sample((n_samples,))\n",
    "        # Transform the sampled latent variables if necessary\n",
    "        z = self.z_encoder.z_transformation(untran_z)\n",
    "        # Compute the library by taking log of the observed library size\n",
    "        library = torch.log(observed_lib_size)\n",
    "        if n_samples > 1:\n",
    "            library = library.unsqueeze(0).expand((n_samples, library.size(0), library.size(1)))\n",
    "\n",
    "        return {\n",
    "            MODULE_KEYS.Z_KEY: z,\n",
    "            MODULE_KEYS.QZ_KEY: qz,\n",
    "            MODULE_KEYS.QL_KEY: None,\n",
    "            MODULE_KEYS.LIBRARY_KEY: library,\n",
    "        }\n",
    "\n",
    "    @auto_move_data\n",
    "\n",
    "# 8. Decodes latent z back to gene expression\n",
    "    def generative(\n",
    "        self,\n",
    "        z: torch.Tensor,                   # Latent variable samples\n",
    "        library: torch.Tensor,             # Library size estimates or samples\n",
    "        batch_index: torch.Tensor,         # Batch indices for each cell\n",
    "        cont_covs: torch.Tensor | None = None,  # Continuous covariates (if any)\n",
    "        cat_covs: torch.Tensor | None = None,   # Categorical covariates (if any)\n",
    "        size_factor: torch.Tensor | None = None,  # Size factor (if provided)\n",
    "        y: torch.Tensor | None = None,      # Labels (if provided)\n",
    "        transform_batch: torch.Tensor | None = None,  # Optional transformation for batch indices\n",
    "    ) -> dict[str, Distribution | None]:\n",
    "        \"\"\"Run the generative process.\"\"\"\n",
    "        from torch.nn.functional import linear\n",
    "        from scvi.distributions import NegativeBinomial, Normal, Poisson, ZeroInflatedNegativeBinomial\n",
    "\n",
    "        # Prepare the decoder input by concatenating z with continuous covariates if provided\n",
    "        if cont_covs is None:\n",
    "            decoder_input = z\n",
    "        elif z.dim() != cont_covs.dim():\n",
    "            decoder_input = torch.cat(\n",
    "                [z, cont_covs.unsqueeze(0).expand(z.size(0), -1, -1)], dim=-1\n",
    "            )\n",
    "        else:\n",
    "            decoder_input = torch.cat([z, cont_covs], dim=-1)\n",
    "\n",
    "        # Prepare categorical covariate inputs\n",
    "        if cat_covs is not None:\n",
    "            categorical_input = torch.split(cat_covs, 1, dim=1)\n",
    "        else:\n",
    "            categorical_input = ()\n",
    "\n",
    "        # If a transform_batch is provided, override the batch_index with the transformed value\n",
    "        if transform_batch is not None:\n",
    "            batch_index = torch.ones_like(batch_index) * transform_batch\n",
    "\n",
    "        # Determine size_factor: if not using a size factor key, then library is used directly\n",
    "        if not self.use_size_factor_key:\n",
    "            size_factor = library\n",
    "\n",
    "        # If using batch embedding representation, compute the batch representation and concatenate\n",
    "        if self.batch_representation == \"embedding\":\n",
    "            batch_rep = self.compute_embedding(REGISTRY_KEYS.BATCH_KEY, batch_index)\n",
    "            decoder_input = torch.cat([decoder_input, batch_rep], dim=-1)\n",
    "            # Call the decoder with the provided inputs and unpack its outputs\n",
    "            px_scale, px_r, px_rate, px_dropout = self.decoder(\n",
    "                self.dispersion,\n",
    "                decoder_input,\n",
    "                size_factor,\n",
    "                *categorical_input,\n",
    "                y,\n",
    "            )\n",
    "        else:\n",
    "            # For one-hot batch representation, pass batch_index directly to the decoder\n",
    "            px_scale, px_r, px_rate, px_dropout = self.decoder(\n",
    "                self.dispersion,\n",
    "                decoder_input,\n",
    "                size_factor,\n",
    "                batch_index,\n",
    "                *categorical_input,\n",
    "                y,\n",
    "            )\n",
    "\n",
    "        # Adjust px_r based on the type of dispersion chosen\n",
    "        if self.dispersion == \"gene-label\":\n",
    "            px_r = linear(\n",
    "                one_hot(y.squeeze(-1), self.n_labels).float(), self.px_r\n",
    "            )  # Applies a linear transformation after one-hot encoding the labels\n",
    "        elif self.dispersion == \"gene-batch\":\n",
    "            px_r = linear(one_hot(batch_index.squeeze(-1), self.n_batch).float(), self.px_r)\n",
    "        elif self.dispersion == \"gene\":\n",
    "            px_r = self.px_r\n",
    "\n",
    "        px_r = torch.exp(px_r)  # Exponentiate to ensure positive dispersion parameter\n",
    "\n",
    "        # Based on the specified gene likelihood, create the appropriate distribution\n",
    "        if self.gene_likelihood == \"zinb\":\n",
    "            px = ZeroInflatedNegativeBinomial(\n",
    "                mu=px_rate,\n",
    "                theta=px_r,\n",
    "                zi_logits=px_dropout,\n",
    "                scale=px_scale,\n",
    "            )\n",
    "        elif self.gene_likelihood == \"nb\":\n",
    "            px = NegativeBinomial(mu=px_rate, theta=px_r, scale=px_scale)\n",
    "        elif self.gene_likelihood == \"poisson\":\n",
    "            px = Poisson(rate=px_rate, scale=px_scale)\n",
    "        elif self.gene_likelihood == \"normal\":\n",
    "            px = Normal(px_rate, px_r, normal_mu=px_scale)\n",
    "\n",
    "        # Priors for library size and latent variable z\n",
    "        if self.use_observed_lib_size:\n",
    "            pl = None  # No prior on library size if observed\n",
    "        else:\n",
    "            # Compute local library parameters based on batch indices\n",
    "            local_library_log_means, local_library_log_vars = self._compute_local_library_params(batch_index)\n",
    "            pl = Normal(local_library_log_means, local_library_log_vars.sqrt())\n",
    "        pz = Normal(torch.zeros_like(z), torch.ones_like(z))  # Standard normal prior for latent variable z\n",
    "\n",
    "        return {\n",
    "            MODULE_KEYS.PX_KEY: px,  # Likelihood distribution for gene expression\n",
    "            MODULE_KEYS.PL_KEY: pl,  # Prior distribution for library size\n",
    "            MODULE_KEYS.PZ_KEY: pz,  # Prior distribution for latent variable z\n",
    "        }\n",
    "\n",
    "    @unsupported_if_adata_minified  # Mark this method as unsupported if AnnData is in minified mode\n",
    "\n",
    "# 9. Computes total VAE loss\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],  # Dictionary containing input tensors (e.g., gene expression)\n",
    "        inference_outputs: dict[str, torch.Tensor | Distribution | None],  # Outputs from the inference process\n",
    "        generative_outputs: dict[str, Distribution | None],  # Outputs from the generative process\n",
    "        kl_weight: torch.tensor | float = 1.0,  # Weight to scale the KL divergence term\n",
    "    ) -> LossOutput:\n",
    "        \"\"\"Compute the loss.\"\"\"\n",
    "        from torch.distributions import kl_divergence\n",
    "\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]  # Retrieve the observed gene expression data\n",
    "        # Compute KL divergence for the latent variable z\n",
    "        kl_divergence_z = kl_divergence(\n",
    "            inference_outputs[MODULE_KEYS.QZ_KEY], generative_outputs[MODULE_KEYS.PZ_KEY]\n",
    "        ).sum(dim=-1)\n",
    "        # Compute KL divergence for the library size l if not using observed library size\n",
    "        if not self.use_observed_lib_size:\n",
    "            kl_divergence_l = kl_divergence(\n",
    "                inference_outputs[MODULE_KEYS.QL_KEY], generative_outputs[MODULE_KEYS.PL_KEY]\n",
    "            ).sum(dim=1)\n",
    "        else:\n",
    "            kl_divergence_l = torch.zeros_like(kl_divergence_z)\n",
    "\n",
    "        # Compute reconstruction loss as the negative log-likelihood of the observed data\n",
    "        reconst_loss = -generative_outputs[MODULE_KEYS.PX_KEY].log_prob(x).sum(-1)\n",
    "\n",
    "        # Combine the KL divergence components with the reconstruction loss\n",
    "        kl_local_for_warmup = kl_divergence_z\n",
    "        kl_local_no_warmup = kl_divergence_l\n",
    "\n",
    "        weighted_kl_local = kl_weight * kl_local_for_warmup + kl_local_no_warmup\n",
    "\n",
    "        loss = torch.mean(reconst_loss + weighted_kl_local)\n",
    "\n",
    "        # Return a structured loss output with extra metrics for monitoring\n",
    "        return LossOutput(\n",
    "            loss=loss,\n",
    "            reconstruction_loss=reconst_loss,\n",
    "            kl_local={\n",
    "                MODULE_KEYS.KL_L_KEY: kl_divergence_l,\n",
    "                MODULE_KEYS.KL_Z_KEY: kl_divergence_z,\n",
    "            },\n",
    "            extra_metrics={\n",
    "                \"z\": inference_outputs[\"z\"],\n",
    "                \"batch\": tensors[REGISTRY_KEYS.BATCH_KEY],\n",
    "                \"labels\": tensors[REGISTRY_KEYS.LABELS_KEY],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "\n",
    "# 10. Samples gene expression from the posterior predictive distribution\n",
    "    def sample(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],  # Input tensors for sampling\n",
    "        n_samples: int = 1,                  # Number of Monte Carlo samples to draw per observation\n",
    "        max_poisson_rate: float = 1e8,       # Maximum value to clip Poisson rate to avoid numerical issues\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Generate predictive samples from the posterior predictive distribution.\n",
    "\n",
    "        The posterior predictive distribution is denoted as :math:`p(\\hat{x} \\mid x)`, where\n",
    "        :math:`x` is the input data and :math:`\\hat{x}` is the sampled data.\n",
    "\n",
    "        We sample from this distribution by first sampling ``n_samples`` times from the posterior\n",
    "        distribution :math:`q(z \\mid x)` for a given observation, and then sampling from the\n",
    "        likelihood :math:`p(\\hat{x} \\mid z)` for each of these.\n",
    "        \"\"\"\n",
    "        from scvi.distributions import Poisson\n",
    "\n",
    "        inference_kwargs = {\"n_samples\": n_samples}\n",
    "        # Run a forward pass to get generative outputs (without computing loss)\n",
    "        _, generative_outputs = self.forward(\n",
    "            tensors, inference_kwargs=inference_kwargs, compute_loss=False\n",
    "        )\n",
    "\n",
    "        dist = generative_outputs[MODULE_KEYS.PX_KEY]\n",
    "        if self.gene_likelihood == \"poisson\":\n",
    "            # Handle potential issues on MPS devices by clamping the Poisson rate\n",
    "            dist = (\n",
    "                Poisson(torch.clamp(dist.rate.to(\"cpu\"), max=max_poisson_rate))\n",
    "                if self.device.type == \"mps\"\n",
    "                else Poisson(torch.clamp(dist.rate, max=max_poisson_rate))\n",
    "            )\n",
    "\n",
    "        # Draw samples from the likelihood distribution; shape depends on n_samples\n",
    "        samples = dist.sample()\n",
    "        # If multiple samples were drawn, permute dimensions so that output is (n_obs, n_vars, n_samples)\n",
    "        samples = torch.permute(samples, (1, 2, 0)) if n_samples > 1 else samples\n",
    "\n",
    "        return samples.cpu()  # Return samples on CPU\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    @auto_move_data\n",
    "\n",
    "# Estimates marginal log-likelihood with Monte Carlo sampling\n",
    "\n",
    "    def marginal_ll(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],  # Input tensors for marginal likelihood computation\n",
    "        n_mc_samples: int,  # Total number of Monte Carlo samples for estimation\n",
    "        return_mean: bool = False,  # Whether to return the mean marginal likelihood over cells\n",
    "        n_mc_samples_per_pass: int = 1,  # Number of samples per computation pass (to reduce memory usage)\n",
    "    ):\n",
    "        \"\"\"Compute the marginal log-likelihood of the data under the model.\"\"\"\n",
    "        from torch import logsumexp\n",
    "        from torch.distributions import Normal\n",
    "\n",
    "        batch_index = tensors[REGISTRY_KEYS.BATCH_KEY]\n",
    "\n",
    "        to_sum = []  # List to accumulate log probabilities over multiple passes\n",
    "        if n_mc_samples_per_pass > n_mc_samples:\n",
    "            warnings.warn(\n",
    "                \"Number of chunks is larger than the total number of samples, setting it to the \"\n",
    "                \"number of samples\",\n",
    "                RuntimeWarning,\n",
    "                stacklevel=settings.warnings_stacklevel,\n",
    "            )\n",
    "            n_mc_samples_per_pass = n_mc_samples\n",
    "        n_passes = int(np.ceil(n_mc_samples / n_mc_samples_per_pass))\n",
    "        for _ in range(n_passes):\n",
    "            # For each pass, run a forward pass to get inference outputs and loss components\n",
    "            inference_outputs, _, losses = self.forward(\n",
    "                tensors,\n",
    "                inference_kwargs={\"n_samples\": n_mc_samples_per_pass},\n",
    "                get_inference_input_kwargs={\"full_forward_pass\": True},\n",
    "            )\n",
    "            qz = inference_outputs[MODULE_KEYS.QZ_KEY]\n",
    "            ql = inference_outputs[MODULE_KEYS.QL_KEY]\n",
    "            z = inference_outputs[MODULE_KEYS.Z_KEY]\n",
    "            library = inference_outputs[MODULE_KEYS.LIBRARY_KEY]\n",
    "\n",
    "            # Get the reconstruction loss from the losses output\n",
    "            reconst_loss = losses.dict_sum(losses.reconstruction_loss)\n",
    "\n",
    "            # Compute log probabilities for the latent variable and reconstruction\n",
    "            p_z = (\n",
    "                Normal(torch.zeros_like(qz.loc), torch.ones_like(qz.scale)).log_prob(z).sum(dim=-1)\n",
    "            )\n",
    "            p_x_zl = -reconst_loss\n",
    "            q_z_x = qz.log_prob(z).sum(dim=-1)\n",
    "            log_prob_sum = p_z + p_x_zl - q_z_x\n",
    "\n",
    "            if not self.use_observed_lib_size:\n",
    "                # Compute additional log probabilities for library size if not observed\n",
    "                local_library_log_means, local_library_log_vars = self._compute_local_library_params(batch_index)\n",
    "                p_l = (\n",
    "                    Normal(local_library_log_means, local_library_log_vars.sqrt())\n",
    "                    .log_prob(library)\n",
    "                    .sum(dim=-1)\n",
    "                )\n",
    "                q_l_x = ql.log_prob(library).sum(dim=-1)\n",
    "                log_prob_sum += p_l - q_l_x\n",
    "            if n_mc_samples_per_pass == 1:\n",
    "                log_prob_sum = log_prob_sum.unsqueeze(0)\n",
    "\n",
    "            to_sum.append(log_prob_sum)\n",
    "        # Concatenate all passes and compute log-sum-exp for a Monte Carlo estimate\n",
    "        to_sum = torch.cat(to_sum, dim=0)\n",
    "        batch_log_lkl = logsumexp(to_sum, dim=0) - np.log(n_mc_samples)\n",
    "        if return_mean:\n",
    "            batch_log_lkl = torch.mean(batch_log_lkl).item()\n",
    "        else:\n",
    "            batch_log_lkl = batch_log_lkl.cpu()\n",
    "        return batch_log_lkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
