{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88803a91-1fe5-4a7b-bc16-d46c067e24d8",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa031c1-6a12-4fb1-b415-f3ec3372b4d5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ad700-2aab-4ec6-9da0-80bbe42fba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections.abc import Callable, Iterable\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from scvi.nn._utils import ExpActivation\n",
    "\n",
    "def _identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce27351-135a-49df-a3db-fa2a06d74204",
   "metadata": {},
   "source": [
    "## FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598309fd-92fd-4198-a440-c1227dec6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayers(nn.Module):\n",
    "    \"\"\"A helper class to build fully-connected layers for a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_in\n",
    "        The dimensionality of the input\n",
    "    n_out\n",
    "        The dimensionality of the output\n",
    "    n_cat_list\n",
    "        A list containing, for each category of interest,\n",
    "        the number of categories. Each category will be\n",
    "        included using a one-hot encoding.\n",
    "    n_cont\n",
    "        The dimensionality of the continuous covariates\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    use_batch_norm\n",
    "        Whether to have `BatchNorm` layers or not\n",
    "    use_layer_norm\n",
    "        Whether to have `LayerNorm` layers or not\n",
    "    use_activation\n",
    "        Whether to have layer activation or not\n",
    "    bias\n",
    "        Whether to learn bias in linear layers or not\n",
    "    inject_covariates\n",
    "        Whether to inject covariates in each layer, or just the first (default).\n",
    "    activation_fn\n",
    "        Which activation function to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_out: int,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_cont: int = 0,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        dropout_rate: float = 0.1,\n",
    "        use_batch_norm: bool = True,\n",
    "        use_layer_norm: bool = False,\n",
    "        use_activation: bool = True,\n",
    "        bias: bool = True,\n",
    "        inject_covariates: bool = True,\n",
    "        activation_fn: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.inject_covariates = inject_covariates # Stores whether to inject covariates.\n",
    "        layers_dim = [n_in] + (n_layers - 1) * [n_hidden] + [n_out] # Builds a list of layer dimensions. E.g., for 2 layers: [input_dim, hidden_dim, output_dim]\n",
    "\n",
    "        if n_cat_list is not None:\n",
    "            # n_cat = 1 will be ignored\n",
    "            self.n_cat_list = [n_cat if n_cat > 1 else 0 for n_cat in n_cat_list] \n",
    "        else:\n",
    "            self.n_cat_list = []\n",
    "\n",
    "        self.n_cov = n_cont + sum(self.n_cat_list) # Total number of covariate dimensions = continuous + one-hot encoded categorical.\n",
    "\n",
    "        # Builds a sequence of layers (nn.Sequential) for each pair of input-output dimensions.\n",
    "        self.fc_layers = nn.Sequential( \n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        f\"Layer {i}\",\n",
    "                        nn.Sequential(\n",
    "                            nn.Linear( \n",
    "                                n_in + self.n_cov * self.inject_into_layer(i), \n",
    "                                n_out,\n",
    "                                bias=bias,\n",
    "                            ),\n",
    "                            # non-default params come from defaults in original Tensorflow\n",
    "                            # implementation\n",
    "                            nn.BatchNorm1d(n_out, momentum=0.01, eps=0.001)\n",
    "                            if use_batch_norm\n",
    "                            else None,\n",
    "                            nn.LayerNorm(n_out, elementwise_affine=False)\n",
    "                            if use_layer_norm\n",
    "                            else None,\n",
    "                            activation_fn() if use_activation else None,\n",
    "                            nn.Dropout(p=dropout_rate) if dropout_rate > 0 else None,\n",
    "                        ),\n",
    "                    )\n",
    "                    for i, (n_in, n_out) in enumerate(\n",
    "                        zip(layers_dim[:-1], layers_dim[1:], strict=True)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #  Helper for Covariate Injection\n",
    "    def inject_into_layer(self, layer_num) -> bool:\n",
    "        \"\"\"Helper to determine if covariates should be injected.\"\"\"\n",
    "        user_cond = layer_num == 0 or (layer_num > 0 and self.inject_covariates)\n",
    "        return user_cond\n",
    "\n",
    "    # Adds hooks to modify gradients during backpropagation\n",
    "    def set_online_update_hooks(self, hook_first_layer=True):\n",
    "        \"\"\"Set online update hooks.\"\"\"\n",
    "        self.hooks = []\n",
    "\n",
    "        def _hook_fn_weight(grad):\n",
    "            categorical_dims = sum(self.n_cat_list)\n",
    "            new_grad = torch.zeros_like(grad)\n",
    "            if categorical_dims > 0:\n",
    "                new_grad[:, -categorical_dims:] = grad[:, -categorical_dims:]\n",
    "            return new_grad\n",
    "\n",
    "        def _hook_fn_zero_out(grad):\n",
    "            return grad * 0\n",
    "\n",
    "        for i, layers in enumerate(self.fc_layers):\n",
    "            for layer in layers:\n",
    "                if i == 0 and not hook_first_layer:\n",
    "                    continue\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    if self.inject_into_layer(i):\n",
    "                        w = layer.weight.register_hook(_hook_fn_weight)\n",
    "                    else:\n",
    "                        w = layer.weight.register_hook(_hook_fn_zero_out)\n",
    "                    self.hooks.append(w)\n",
    "                    b = layer.bias.register_hook(_hook_fn_zero_out)\n",
    "                    self.hooks.append(b)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, *cat_list: int, cont: torch.Tensor | None = None):\n",
    "        \"\"\"Forward computation on ``x``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(n_in,)``\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "        cont\n",
    "            tensor of continuous covariates with shape ``(n_cont,)``\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`torch.Tensor`\n",
    "            tensor of shape ``(n_out,)``\n",
    "        \"\"\"\n",
    "        one_hot_cat_list = []  # for generality in this list many idxs useless.\n",
    "        cont_list = [cont] if cont is not None else []\n",
    "        cat_list = cat_list or []\n",
    "\n",
    "        if len(self.n_cat_list) > len(cat_list):\n",
    "            raise ValueError(\"nb. categorical args provided doesn't match init. params.\")\n",
    "        for n_cat, cat in zip(self.n_cat_list, cat_list, strict=False):\n",
    "            if n_cat and cat is None:\n",
    "                raise ValueError(\"cat not provided while n_cat != 0 in init. params.\")\n",
    "            if n_cat > 1:  # n_cat = 1 will be ignored - no additional information\n",
    "                if cat.size(1) != n_cat:\n",
    "                    one_hot_cat = nn.functional.one_hot(cat.squeeze(-1), n_cat)\n",
    "                else:\n",
    "                    one_hot_cat = cat  # cat has already been one_hot encoded\n",
    "                one_hot_cat_list += [one_hot_cat]\n",
    "        cov_list = cont_list + one_hot_cat_list\n",
    "\n",
    "        # Iterates through each sequential layer (e.g., linear, activation, etc.)\n",
    "        for i, layers in enumerate(self.fc_layers):\n",
    "            for layer in layers:\n",
    "                if layer is not None:\n",
    "                    # Applies batch norm across batches and handles MPS device quirks (macOS GPU).\n",
    "                    if isinstance(layer, nn.BatchNorm1d):\n",
    "                        if x.dim() == 3:\n",
    "                            if (\n",
    "                                x.device.type == \"mps\"\n",
    "                            ):  # TODO: remove this when MPS supports for loop.\n",
    "                                x = torch.cat(\n",
    "                                    [(layer(slice_x.clone())).unsqueeze(0) for slice_x in x], dim=0\n",
    "                                )\n",
    "                            else:\n",
    "                                x = torch.cat(\n",
    "                                    [layer(slice_x).unsqueeze(0) for slice_x in x], dim=0\n",
    "                                )\n",
    "                        else:\n",
    "                            # Applies the layer (linear, norm, activation, dropout, etc.).\n",
    "                            x = layer(x)\n",
    "                    else:\n",
    "                        # Injects covariates before applying linear transformation.\n",
    "                        if isinstance(layer, nn.Linear) and self.inject_into_layer(i):\n",
    "                            if x.dim() == 3:\n",
    "                                cov_list_layer = [\n",
    "                                    o.unsqueeze(0).expand((x.size(0), o.size(0), o.size(1)))\n",
    "                                    for o in cov_list\n",
    "                                ]\n",
    "                            else:\n",
    "                                cov_list_layer = cov_list\n",
    "                            x = torch.cat((x, *cov_list_layer), dim=-1)\n",
    "                        x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96d0d2-1fe0-4cb5-88c6-c6b3fc4e940b",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b02e0-9683-4e52-a416-49ae88a56c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encode data of ``n_input`` dimensions into a latent space of ``n_output`` dimensions.\n",
    "\n",
    "    Uses a fully-connected neural network of ``n_hidden`` layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        The dimensionality of the input (data space)\n",
    "    n_output\n",
    "        The dimensionality of the output (latent space)\n",
    "    n_cat_list\n",
    "        A list containing the number of categories\n",
    "        for each category of interest. Each category will be\n",
    "        included using a one-hot encoding\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    distribution\n",
    "        Distribution of z\n",
    "    var_eps\n",
    "        Minimum value for the variance;\n",
    "        used for numerical stability\n",
    "    var_activation\n",
    "        Callable used to ensure positivity of the variance.\n",
    "        Defaults to :meth:`torch.exp`.\n",
    "    return_dist\n",
    "        Return directly the distribution of z instead of its parameters.\n",
    "    **kwargs\n",
    "        Keyword args for :class:`~scvi.nn.FCLayers`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        dropout_rate: float = 0.1,\n",
    "        distribution: str = \"normal\",\n",
    "        var_eps: float = 1e-4,\n",
    "        var_activation: Callable | None = None,\n",
    "        return_dist: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.distribution = distribution\n",
    "        self.var_eps = var_eps\n",
    "        self.encoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.mean_encoder = nn.Linear(n_hidden, n_output)\n",
    "        self.var_encoder = nn.Linear(n_hidden, n_output)\n",
    "        self.return_dist = return_dist\n",
    "\n",
    "        if distribution == \"ln\":\n",
    "            self.z_transformation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            self.z_transformation = _identity\n",
    "        self.var_activation = torch.exp if var_activation is None else var_activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *cat_list: int):\n",
    "        r\"\"\"The forward computation for a single sample.\n",
    "\n",
    "         #. Encodes the data into latent space using the encoder network\n",
    "         #. Generates a mean \\\\( q_m \\\\) and variance \\\\( q_v \\\\)\n",
    "         #. Samples a new value from an i.i.d. multivariate normal\n",
    "            \\\\( \\\\sim Ne(q_m, \\\\mathbf{I}q_v) \\\\)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor with shape (n_input,)\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        3-tuple of :py:class:`torch.Tensor`\n",
    "            tensors of shape ``(n_latent,)`` for mean and var, and sample\n",
    "\n",
    "        \"\"\"\n",
    "        # Parameters for latent distribution\n",
    "        q = self.encoder(x, *cat_list)\n",
    "        q_m = self.mean_encoder(q)\n",
    "        q_v = self.var_activation(self.var_encoder(q)) + self.var_eps\n",
    "        dist = Normal(q_m, q_v.sqrt())\n",
    "        latent = self.z_transformation(dist.rsample())\n",
    "        if self.return_dist:\n",
    "            return dist, latent\n",
    "        return q_m, q_v, latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae96349-8351-4c6e-adc9-6324c7564ae8",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc62542-02f9-4c69-a423-806270853455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decoder\n",
    "class DecoderSCVI(nn.Module):\n",
    "    \"\"\"Decodes data from latent space of ``n_input`` dimensions into ``n_output`` dimensions.\n",
    "\n",
    "    Uses a fully-connected neural network of ``n_hidden`` layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        The dimensionality of the input (latent space)\n",
    "    n_output\n",
    "        The dimensionality of the output (data space)\n",
    "    n_cat_list\n",
    "        A list containing the number of categories\n",
    "        for each category of interest. Each category will be\n",
    "        included using a one-hot encoding\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    inject_covariates\n",
    "        Whether to inject covariates in each layer, or just the first (default).\n",
    "    use_batch_norm\n",
    "        Whether to use batch norm in layers\n",
    "    use_layer_norm\n",
    "        Whether to use layer norm in layers\n",
    "    scale_activation\n",
    "        Activation layer to use for px_scale_decoder\n",
    "    **kwargs\n",
    "        Keyword args for :class:`~scvi.nn.FCLayers`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        inject_covariates: bool = True,\n",
    "        use_batch_norm: bool = False,\n",
    "        use_layer_norm: bool = False,\n",
    "        scale_activation: Literal[\"softmax\", \"softplus\"] = \"softmax\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.px_decoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=0,\n",
    "            inject_covariates=inject_covariates,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # mean gamma\n",
    "        if scale_activation == \"softmax\":\n",
    "            px_scale_activation = nn.Softmax(dim=-1)\n",
    "        elif scale_activation == \"softplus\":\n",
    "            px_scale_activation = nn.Softplus()\n",
    "        self.px_scale_decoder = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_output),\n",
    "            px_scale_activation,\n",
    "        )\n",
    "\n",
    "        # dispersion: here we only deal with gene-cell dispersion case\n",
    "        self.px_r_decoder = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "        # dropout\n",
    "        self.px_dropout_decoder = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        dispersion: str,\n",
    "        z: torch.Tensor,\n",
    "        library: torch.Tensor,\n",
    "        *cat_list: int,\n",
    "    ):\n",
    "        \"\"\"The forward computation for a single sample.\n",
    "\n",
    "         #. Decodes the data from the latent space using the decoder network\n",
    "         #. Returns parameters for the ZINB distribution of expression\n",
    "         #. If ``dispersion != 'gene-cell'`` then value for that param will be ``None``\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dispersion\n",
    "            One of the following\n",
    "\n",
    "            * ``'gene'`` - dispersion parameter of NB is constant per gene across cells\n",
    "            * ``'gene-batch'`` - dispersion can differ between different batches\n",
    "            * ``'gene-label'`` - dispersion can differ between different labels\n",
    "            * ``'gene-cell'`` - dispersion can differ for every gene in every cell\n",
    "        z :\n",
    "            tensor with shape ``(n_input,)``\n",
    "        library_size\n",
    "            library size\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        4-tuple of :py:class:`torch.Tensor`\n",
    "            parameters for the ZINB distribution of expression\n",
    "\n",
    "        \"\"\"\n",
    "        # The decoder returns values for the parameters of the ZINB distribution\n",
    "        px = self.px_decoder(z, *cat_list)\n",
    "        px_scale = self.px_scale_decoder(px)\n",
    "        px_dropout = self.px_dropout_decoder(px)\n",
    "        # Clamp to high value: exp(12) ~ 160000 to avoid nans (computational stability)\n",
    "        px_rate = torch.exp(library) * px_scale  # torch.clamp( , max=12)\n",
    "        px_r = self.px_r_decoder(px) if dispersion == \"gene-cell\" else None\n",
    "        return px_scale, px_r, px_rate, px_dropout\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
